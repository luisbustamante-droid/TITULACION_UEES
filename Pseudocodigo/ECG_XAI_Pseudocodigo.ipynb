{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2dd5e4",
   "metadata": {},
   "source": [
    "\n",
    "# Tesis: Clasificación explicable de arritmias cardíacas desde espectrogramas (CNN + Grad-CAM)\n",
    "**Autor:** Luis — **Caderno de trabajo / Pseudocódigo ejecutable**  \n",
    "**Última actualización:** 2025-09-20 01:03\n",
    "\n",
    "Este notebook define **pseudocódigo** (plantillas y funciones en blanco) para implementar el pipeline completo:\n",
    "1. Datos y **split inter-paciente** (anti-leakage).\n",
    "2. Segmentación en **ventanas de 5 s**.\n",
    "3. Transformación a **espectrogramas** (STFT multi-resolución).\n",
    "4. **Modelo** (ResNet-18 2D con opción de SE/CBAM).\n",
    "5. **Entrenamiento** con pérdida ponderada / Focal Loss.\n",
    "6. **Calibración** (Temperature Scaling; métricas ECE/Brier).\n",
    "7. **Explicabilidad** (Grad-CAM, sanity checks, ROAR).\n",
    "8. **Robustez** a ruido/artefactos (SNR 20/10 dB).\n",
    "9. **Evaluación** (macro-F1, matriz de confusión, ICs).\n",
    "10. **Preparación UI** (artefactos para Streamlit).\n",
    "\n",
    "> **Nota:** Este cuaderno prioriza claridad. Muchas funciones están en *pseudocódigo* (con `pass` o comentarios) para guiar la implementación real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243336e",
   "metadata": {},
   "source": [
    "\n",
    "## Índice\n",
    "- [0. Configuración & Objetivos](#0)\n",
    "- [1. Datos & Particiones inter-paciente](#1)\n",
    "- [2. Segmentación & Transformaciones (STFT)](#2)\n",
    "- [3. Modelo (ResNet18 + SE/CBAM)](#3)\n",
    "- [4. Entrenamiento](#4)\n",
    "- [5. Calibración (ECE/Brier)](#5)\n",
    "- [6. Explicabilidad (Grad-CAM, ROAR)](#6)\n",
    "- [7. Robustez a ruido](#7)\n",
    "- [8. Evaluación & Reportes](#8)\n",
    "- [9. Export para Streamlit](#9)\n",
    "\n",
    "### Checklist anti-errores\n",
    "- [ ] Split **inter-paciente** (sin sujetos repetidos).\n",
    "- [ ] **AAMI mapping** fijo (N, S, V, F, Q).\n",
    "- [ ] Normalizadores ajustados **solo con train**.\n",
    "- [ ] CLase balanceada: pesos o **Focal Loss**.\n",
    "- [ ] Semillas y versiones registradas.\n",
    "- [ ] Calibración (Temperature Scaling) aplicada en test.\n",
    "- [ ] XAI: sanity checks + **ROAR**.\n",
    "- [ ] Robustez: SNR 20/10 dB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fbfe6a",
   "metadata": {},
   "source": [
    "## <a id='0'></a>0) Configuración & Objetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a721fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 0) CONFIGURACIÓN & OBJETIVOS ===\n",
    "# Este bloque define constantes y metas del experimento.\n",
    "\n",
    "EXPERIMENT_NAME = \"ecg_xai_stft_multires\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Objetivo principal (medible):\n",
    "TARGET_MACRO_F1 = 0.90         # esperado en split inter-paciente\n",
    "TARGET_ECE       = 0.05        # calibración tras temperature scaling\n",
    "ACCEPTABLE_DELTA_F1_ROBUSTEZ = 0.10  # caída <=10% con SNR 10-20 dB\n",
    "\n",
    "# Clases finales (mapeo AAMI)\n",
    "CLASSES = [\"N\", \"S\", \"V\", \"F\", \"Q\"]\n",
    "\n",
    "# Parámetros de segmentación\n",
    "WINDOW_SECONDS = 5.0\n",
    "SAMPLE_RATE = 360  # típico de MIT-BIH; ajustar si difiere\n",
    "\n",
    "# Espectrograma (multi-res)\n",
    "STFT_NFFTS = [256, 512]   # resoluciones\n",
    "HOP_LENGTH = 128          # ajustar tras pruebas\n",
    "WINDOW_FUNC = \"hamming\"   # o hann\n",
    "\n",
    "# Entrenamiento (placeholders)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 100\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "\n",
    "# Notas: Las implementaciones reales se agregarán en módulos/archivos aparte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fd353",
   "metadata": {},
   "source": [
    "## <a id='1'></a>1) Datos & Particiones inter-paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad39d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1) DATOS & SPLITS ===\n",
    "# Pseudocódigo: carga de MIT-BIH / PhysioNet, mapeo AAMI y split por paciente.\n",
    "# Reemplazar 'pass' por código real y usar pruebas de verificación (asserts).\n",
    "\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "def load_mitbih_dataset(data_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Cargar señales y anotaciones por PACIENTE (no por latido).\n",
    "    Retorno sugerido:\n",
    "      {\n",
    "        'patients': List[str],\n",
    "        'signals': Dict[patient_id, np.ndarray shape (n_samples, n_leads)],\n",
    "        'ann': Dict[patient_id, List[annotations]],\n",
    "        'fs': Dict[patient_id, int]  # frecuencia por paciente (si varía)\n",
    "      }\n",
    "    \"\"\"\n",
    "    # TODO: implementar con wfdb o formato disponible.\n",
    "    # Importante: no mezclar info de distintos pacientes aquí.\n",
    "    pass\n",
    "\n",
    "def aami_label_map(annotation: Any) -> str:\n",
    "    \"\"\"\n",
    "    Mapear anotación original -> etiqueta AAMI (N, S, V, F, Q).\n",
    "    Documentar explícitamente el mapeo utilizado.\n",
    "    \"\"\"\n",
    "    # TODO: implementar mapeo exacto según guía AAMI/PhysioNet.\n",
    "    pass\n",
    "\n",
    "def make_subject_splits(patients: List[str], ratios=(0.6, 0.2, 0.2)) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Particiona por sujeto (inter-paciente). No permitir solapamiento.\n",
    "    Retorna {'train': [...], 'val': [...], 'test': [...]}\n",
    "    \"\"\"\n",
    "    # TODO: barajar con semilla fija y dividir.\n",
    "    # assert: intersección entre splits debe ser vacía.\n",
    "    pass\n",
    "\n",
    "def verify_no_leakage(splits: Dict[str, List[str]]) -> None:\n",
    "    \"\"\"\n",
    "    Verifica que no hay pacientes repetidos entre train/val/test.\n",
    "    \"\"\"\n",
    "    # TODO: assert de conjuntos disjuntos.\n",
    "    pass\n",
    "\n",
    "# Ejemplo de uso (pseudocódigo):\n",
    "# ds = load_mitbih_dataset(\"/path/a/datos\")\n",
    "# splits = make_subject_splits(ds['patients'])\n",
    "# verify_no_leakage(splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1edb9b",
   "metadata": {},
   "source": [
    "## <a id='2'></a>2) Segmentación & Transformaciones (STFT multi-res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 2) SEGMENTACIÓN & STFT ===\n",
    "# Pseudocódigo de segmentación en ventanas de 5s y transformación a espectrogramas multi-res.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def segment_signal(signal: np.ndarray, fs: int, window_sec: float, overlap: float=0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Corta 'signal' en ventanas de longitud window_sec con overlap opcional.\n",
    "    Retorna: np.ndarray shape (num_windows, window_samples)\n",
    "    \"\"\"\n",
    "    # TODO: implementar cortes por índices.\n",
    "    pass\n",
    "\n",
    "def window_label_majority(annotations: List[Any], window: Tuple[int, int]) -> str:\n",
    "    \"\"\"\n",
    "    Etiquetar la ventana por mayoría de latidos (mapeados a AAMI) presentes en el rango [start, end).\n",
    "    \"\"\"\n",
    "    # TODO: contar eventos por clase y devolver la mayoritaria (resolución de empates).\n",
    "    pass\n",
    "\n",
    "def stft_spectrogram(x: np.ndarray, n_fft: int, hop_length: int, window: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Genera espectrograma (magnitud log) para una señal 1D.\n",
    "    Retorna: np.ndarray (freq_bins, time_frames)\n",
    "    \"\"\"\n",
    "    # TODO: usar biblioteca (scipy.signal o torch/STFT) en implementación real.\n",
    "    pass\n",
    "\n",
    "def multires_spectrogram(windowed_signal: np.ndarray, nffts: List[int], hop_length: int, window: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apila varias resoluciones como canales: (C, F, T)\n",
    "    \"\"\"\n",
    "    # TODO: llamar stft_spectrogram por cada n_fft y apilar.\n",
    "    pass\n",
    "\n",
    "def build_dataset_tensors(ds: Dict[str, Any], splits: Dict[str, List[str]]) -> Tuple[Any, Any, Any]:\n",
    "    \"\"\"\n",
    "    Construye tensores (X, y) por split:\n",
    "      X: [N, C, H, W]  (C = nº resoluciones)\n",
    "      y: [N]           (etiquetas AAMI)\n",
    "    \"\"\"\n",
    "    # TODO: segmentar por paciente, etiquetar, transformar a multi-res y normalizar por ventana.\n",
    "    pass\n",
    "\n",
    "# Quick checks que debes implementar en la vida real:\n",
    "# - Visualizar un espectrograma de ejemplo (matshow/imshow).\n",
    "# - Verificar dimensiones: canales= len(STFT_NFFTS).\n",
    "# - Confirmar distribución de clases tras segmentación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684dcfd",
   "metadata": {},
   "source": [
    "## <a id='3'></a>3) Modelo (ResNet18 2D con opción SE/CBAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3) MODELO ===\n",
    "# Pseudocódigo de arquitectura. En la implementación real, usa PyTorch y módulos existentes.\n",
    "\n",
    "class ECGResNet18:\n",
    "    def __init__(self, in_channels: int, num_classes: int, use_se: bool=True):\n",
    "        \"\"\"\n",
    "        Inicializa una ResNet-18 2D ajustada a in_channels.\n",
    "        Si use_se=True, insertar bloques Squeeze-and-Excitation (SE).\n",
    "        \"\"\"\n",
    "        # TODO: construir backbone, adaptar primera conv a in_channels, insertar SE si aplica.\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [N, C, H, W] -> logits [N, num_classes]\n",
    "        \"\"\"\n",
    "        # TODO: forward del backbone + pool + fc.\n",
    "        pass\n",
    "\n",
    "    # Hooks para Grad-CAM (guardar activaciones y gradientes)\n",
    "    def register_cam_hooks(self):\n",
    "        # TODO: registrar forward/backward hooks en el último bloque conv.\n",
    "        pass\n",
    "\n",
    "def focal_loss(logits, targets, alpha_per_class, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal loss con pesos por clase.\n",
    "    \"\"\"\n",
    "    # TODO: implementar focal o usar CE ponderada en la versión real.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf14a",
   "metadata": {},
   "source": [
    "## <a id='4'></a>4) Entrenamiento (loop, early stopping, logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 4) ENTRENAMIENTO ===\n",
    "# Pseudocódigo del bucle: entrenamiento/validación, early stopping por macro-F1.\n",
    "\n",
    "def compute_class_weights(y_train: np.ndarray, classes: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula pesos inversos a la frecuencia por clase.\n",
    "    \"\"\"\n",
    "    # TODO: conteos y normalización.\n",
    "    pass\n",
    "\n",
    "def macro_f1(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    Calcula F1 macro.\n",
    "    \"\"\"\n",
    "    # TODO: usar sklearn en la implementación real.\n",
    "    pass\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Entrena un epoch. Retorna métricas de entrenamiento.\n",
    "    \"\"\"\n",
    "    # TODO: iterar batches, backprop, logs.\n",
    "    pass\n",
    "\n",
    "def evaluate(model, val_loader) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evalúa macro-F1, matriz de confusión provisional, etc.\n",
    "    \"\"\"\n",
    "    # TODO: desactivar gradientes, acumular predicciones, calcular métricas.\n",
    "    pass\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, loss_fn, max_epochs, early_stop_patience) -> Any:\n",
    "    \"\"\"\n",
    "    Entrena con early stopping sobre macro-F1 de validación.\n",
    "    Devuelve el mejor checkpoint (o estado del modelo).\n",
    "    \"\"\"\n",
    "    # TODO: implementar con control de paciencia y guardado del mejor.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d152f0",
   "metadata": {},
   "source": [
    "## <a id='5'></a>5) Calibración (ECE/Brier; Temperature Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 5) CALIBRACIÓN ===\n",
    "# Pseudocódigo para ECE/Brier y temperature scaling.\n",
    "\n",
    "def softmax(logits):\n",
    "    # TODO: implementar softmax estable numéricamente.\n",
    "    pass\n",
    "\n",
    "def expected_calibration_error(probs, labels, n_bins=15) -> float:\n",
    "    \"\"\"\n",
    "    ECE clásico por bins de confianza.\n",
    "    \"\"\"\n",
    "    # TODO: discretizar confianzas y calcular gap promedio ponderado.\n",
    "    pass\n",
    "\n",
    "def brier_score(probs, labels) -> float:\n",
    "    \"\"\"\n",
    "    Brier score multicategoría.\n",
    "    \"\"\"\n",
    "    # TODO: implementar según definición.\n",
    "    pass\n",
    "\n",
    "class TemperatureScaler:\n",
    "    def __init__(self):\n",
    "        self.T = 1.0\n",
    "\n",
    "    def fit(self, logits_val, labels_val):\n",
    "        \"\"\"\n",
    "        Optimiza T para minimizar NLL en validación.\n",
    "        \"\"\"\n",
    "        # TODO: optimización 1D de T (>0), p.ej. con grid/búsqueda o LBFGS.\n",
    "        pass\n",
    "\n",
    "    def transform_logits(self, logits):\n",
    "        return logits / max(self.T, 1e-6)\n",
    "\n",
    "# Flujo típico:\n",
    "# 1) Obtener logits en val\n",
    "# 2) Ajustar T\n",
    "# 3) Aplicar a test y recalcular ECE/Brier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e054cb2",
   "metadata": {},
   "source": [
    "## <a id='6'></a>6) Explicabilidad (Grad-CAM, sanity checks, ROAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4eb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6) EXPLICABILIDAD ===\n",
    "\n",
    "def grad_cam(model, x_batch) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Genera mapas Grad-CAM para el último bloque conv.\n",
    "    Retorna heatmaps con shape [N, H, W].\n",
    "    \"\"\"\n",
    "    # TODO: extraer activaciones y gradientes, combinar y normalizar.\n",
    "    pass\n",
    "\n",
    "def sanity_checks_cam(model, x_batch):\n",
    "    \"\"\"\n",
    "    Sanity checks de Adebayo:\n",
    "      - Re-inicializar pesos y verificar que los mapas cambian.\n",
    "      - Barajar etiquetas y verificar pérdida de estructura.\n",
    "    \"\"\"\n",
    "    # TODO: implementar variantes y aserciones/plots.\n",
    "    pass\n",
    "\n",
    "def roar_fidelity_test(model, dataset, k_percent_list=[10,20,30]):\n",
    "    \"\"\"\n",
    "    ROAR: RemOve And Retrain (o Remove And Retrain-lite si no reentrenas).\n",
    "    Borrado de top-k% regiones según CAM vs borrado aleatorio; comparar F1.\n",
    "    \"\"\"\n",
    "    # TODO: generar máscaras, borrar regiones, medir caída de F1.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbec39",
   "metadata": {},
   "source": [
    "## <a id='7'></a>7) Robustez a ruido/artefactos (SNR 20/10 dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6db0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 7) ROBUSTEZ ===\n",
    "\n",
    "def add_noise(signal: np.ndarray, snr_db: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Añade ruido blanco para alcanzar SNR objetivo (dB).\n",
    "    \"\"\"\n",
    "    # TODO: calcular potencia y escalar ruido.\n",
    "    pass\n",
    "\n",
    "def robustness_benchmark(model, X_test, y_test, snrs=[20, 10]):\n",
    "    \"\"\"\n",
    "    Evalúa Δ-macro-F1 para distintos niveles de SNR.\n",
    "    \"\"\"\n",
    "    # TODO: generar versiones ruidosas y medir F1.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f919c",
   "metadata": {},
   "source": [
    "## <a id='8'></a>8) Evaluación & Reportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60865d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 8) EVALUACIÓN & REPORTES ===\n",
    "\n",
    "def bootstrap_confidence_intervals(y_true, y_pred, n_boot=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Intervalos de confianza por bootstrap (por paciente si es posible).\n",
    "    \"\"\"\n",
    "    # TODO: remuestreo y percentiles (p.ej. 2.5/97.5).\n",
    "    pass\n",
    "\n",
    "def full_evaluation_report(model, test_loader, logits_val=None, labels_val=None):\n",
    "    \"\"\"\n",
    "    1) Métricas macro-F1, por clase, matriz de confusión.\n",
    "    2) Calibración (ECE/Brier) antes/después de TS.\n",
    "    3) Robustez Δ-F1 (SNR).\n",
    "    4) Muestras con Grad-CAM (aciertos/errores).\n",
    "    5) Tiempos (preproc + infer + CAM).\n",
    "    \"\"\"\n",
    "    # TODO: ensamblar, guardar gráficas y JSON/CSV con resultados.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17009f40",
   "metadata": {},
   "source": [
    "## <a id='9'></a>9) Export para Streamlit (artefactos y contrato de I/O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 9) EXPORT PARA STREAMLIT ===\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def export_for_streamlit(best_checkpoint_path: str, tf_params: dict, class_names: List[str]):\n",
    "    \"\"\"\n",
    "    Guarda:\n",
    "      - checkpoint del modelo\n",
    "      - parámetros de TF (n_fft/hop/ventana)\n",
    "      - normalizadores/estadísticos\n",
    "      - etiquetas/clases\n",
    "    Formato sugerido: carpeta 'artefacts/' con JSON/YAML + pesos.\n",
    "    \"\"\"\n",
    "    # TODO: serialización con torch.save / joblib / JSON.\n",
    "    pass\n",
    "\n",
    "STREAMLIT_IO_CONTRACT = {\n",
    "    \"input\": {\n",
    "        \"tipo\": [\"PNG escaneado\", \"WFDB si disponible\"],\n",
    "        \"ventana_segundos\": WINDOW_SECONDS,\n",
    "        \"param_tf\": {\"nffts\": STFT_NFFTS, \"hop_length\": HOP_LENGTH, \"window\": WINDOW_FUNC}\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"pred_clase\": \"string AAMI\",\n",
    "        \"confianza_calibrada\": \"float [0,1]\",\n",
    "        \"grad_cam_png\": \"ruta\",\n",
    "        \"alerta_OOD\": \"bool\",\n",
    "        \"reporte_pdf\": \"ruta opcional\"\n",
    "    }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
