{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7451651-c857-4eb8-a61a-a6cbd4f17f77",
   "metadata": {},
   "source": [
    "# EDA — MIT-BIH Arrhythmia Database\n",
    "\n",
    "## Descripción del Proyecto\n",
    "Este notebook realiza un **Análisis Exploratorio de Datos (EDA)** sobre el dataset **MIT-BIH Arrhythmia Database**.\n",
    "\n",
    "- **Nombre del proyecto**: *Clasificación explicable de arritmias cardíacas a partir de electrocardiogramas transformados en espectrogramas mediante redes neuronales convolucionales*\n",
    "- **Dataset**: MIT-BIH Arrhythmia (señales y anotaciones de ECG)\n",
    "- **Objetivo del EDA**: Identificar patrones y anomalías en las señales de ECG que permitan guiar el desarrollo de un modelo de IA para la clasificación de arritmias.\n",
    "\n",
    "## Objetivos Específicos\n",
    "1. Explorar y comprender la estructura del dataset.\n",
    "2. Analizar distribuciones univariadas y bivariadas de las variables.\n",
    "3. Detectar correlaciones entre variables relevantes.\n",
    "4. Identificar patrones multivariados y posibles anomalías.\n",
    "5. Extraer **insights** que orienten el preprocesamiento y modelado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf08c2-8a3f-409f-bd88-be4308de0b7d",
   "metadata": {},
   "source": [
    "# Explicación del Dataset MIT-BIH\n",
    "\n",
    "Contexto\n",
    "El dataset proviene del **MIT-BIH Arrhythmia Database**, un estándar en cardiología computacional para el diagnóstico automático de arritmias.  \n",
    "Contiene registros de **electrocardiogramas (ECG)** tomados con monitores Holter a 360 Hz, con 2 derivaciones principales y anotaciones clínicas realizadas por cardiólogos.\n",
    "\n",
    "## Variables del Dataset\n",
    "\n",
    "#### Señales de ECG (features)\n",
    "Los nombres de columnas como **`MLII`, `V1`, `V2`, `V4`, `V5`, `V6`** corresponden a **derivaciones estándar del electrocardiograma**:\n",
    "\n",
    "- **MLII (Lead II modificado):** derivación más usada en el MIT-BIH, muy informativa para detectar arritmias.  \n",
    "- **V1, V2, V4, V5, V6:** derivaciones precordiales colocadas en distintas posiciones del pecho, capturan la actividad eléctrica del corazón desde diferentes ángulos.\n",
    "\n",
    "Estas variables representan las **señales crudas de ECG** que servirán como entrada al modelo de IA.\n",
    "\n",
    "#### Variable objetivo (`type`)\n",
    "La columna **`type`** contiene las etiquetas que clasifican cada latido, anotadas por cardiólogos.  \n",
    "Se siguen las categorías de la **AAMI (Association for the Advancement of Medical Instrumentation):**\n",
    "\n",
    "- **N:** Latido normal (Normal beat).  \n",
    "- **S:** Latido supraventricular ectópico (ej. prematuro auricular).  \n",
    "- **V:** Latido ventricular ectópico (ej. PVC).  \n",
    "- **F:** Latido de fusión (mezcla entre normal y ventricular).  \n",
    "- **Q:** Latidos no clasificables o desconocidos.  \n",
    "\n",
    "\n",
    "\n",
    "## Relación entre variables\n",
    "\n",
    "| Variables (`features`) | Qué representan | Uso |\n",
    "|-------------------------|-----------------|-----|\n",
    "| `MLII`, `V1`, `V2`, `V4`, `V5`, `V6` | Señales crudas de ECG en distintas derivaciones | Entrada (input) al modelo |\n",
    "| `type` | Clase del latido (N, S, V, F, Q) | Salida (target) del modelo |\n",
    "\n",
    "\n",
    "## Implicaciones para el modelo\n",
    "- El modelo debe aprender a **mapear patrones en las señales de ECG** hacia una clasificación en `type`.  \n",
    "- Los *features* pueden provenir de:\n",
    "  - **Señales crudas:** ventanas de ECG alrededor de cada latido.  \n",
    "  - **Features handcrafted:** amplitudes, duración de ondas QRS, intervalos RR, etc.  \n",
    "  - **Transformaciones:** wavelets, espectrogramas o embeddings generados con redes neuronales (CNN/RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de812b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Opcional) instala faltantes\n",
    "%pip install statsmodels pingouin numpy pandas matplotlib seaborn scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317bc3c",
   "metadata": {},
   "source": [
    "## Importación de librerías y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23032442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "except Exception:\n",
    "    sm = None\n",
    "    multipletests = None\n",
    "    warnings.warn(\"statsmodels no disponible; se continuará sin algunas pruebas.\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"Versions -> pandas:\", pd.__version__, \"| numpy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c8f31",
   "metadata": {},
   "source": [
    "### Patch · Configuración visual estándar\n",
    "Para homogeneizar todas las figuras del EDA se ajusta el tamaño por defecto y, si está disponible, una paleta consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "# Si usas seaborn en este notebook, aplica una paleta consistente (no es obligatorio).\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_palette('tab10')\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"Configuración de figuras aplicada: figsize=(7,4).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8844f-49bc-4edf-a238-80761c3a71c7",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5137987",
   "metadata": {},
   "source": [
    "## 1.1 Carga de datos y exploración inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0fc1e-ec82-4036-a5ab-55747b7a4c3c",
   "metadata": {},
   "source": [
    "### Configuración: ruta de datos y frecuencia de muestreo (FS)\n",
    "\n",
    "- **`DATA_DIR`**: carpeta donde están los archivos del dataset.  \n",
    "  Si la ruta no existe, el bloque **auto-detecta** una carpeta que contenga archivos tipo `NNN.csv` y `NNNannotations.txt` y la usa.\n",
    "\n",
    "- **`FS = 360` Hz**: frecuencia de muestreo de la base **MIT-BIH Arrhythmia** original  \n",
    "  (≈ **1 muestra cada 2.78 ms**). Se usa para:\n",
    "  - Convertir `sample → time_sec` (`time_sec = sample / FS`).\n",
    "  - Traducir ventanas en milisegundos a **número de muestras** (p. ej., `win_ms`).\n",
    "  > Si `FS` es incorrecto, tiempos y ventanas quedan mal escalados.\n",
    "\n",
    "**¿Por qué 360 Hz?**  \n",
    "Porque los registros estándar de MIT-BIH fueron adquiridos a **360 Hz**. Algunas copias/derivados pueden estar **re-muestreados**; si ese es tu caso, ajusta `FS`.\n",
    "\n",
    "**Chequeo rápido (opcional) si el TXT trae `Time` real:**\n",
    "```python\n",
    "# Estimar FS comparando rango de samples vs rango de tiempo real\n",
    "t = pd.to_timedelta(ann['Time']).dt.total_seconds()\n",
    "FS_est = round((sig['sample'].max() - sig['sample'].min()) / (t.max() - t.min()))\n",
    "print(\"FS estimado:\", FS_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b355259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración\n",
    "DATA_DIR = Path(r\"C:\\Users\\luisa\\Documents\\Titulacion_UEES\\Notebooks\\EDA-MIT-BIH-Arrhythmia-Database\\mit-bih-arrhythmia_dataset\")\n",
    "FS = 360  # Hz\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    candidates = []\n",
    "    for p in Path(\".\").rglob(\"*\"):\n",
    "        try:\n",
    "            if p.is_dir():\n",
    "                has_csv = any(Path(p).glob(\"[0-9][0-9][0-9].csv\"))\n",
    "                has_ann = any(Path(p).glob(\"[0-9][0-9][0-9]annotations.txt\"))\n",
    "                if has_csv and has_ann:\n",
    "                    candidates.append(p)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if candidates:\n",
    "        DATA_DIR = candidates[0]\n",
    "        print(\"DATA_DIR auto-detectado en:\", DATA_DIR.resolve())\n",
    "    else:\n",
    "        print(\"⚠️ Ajusta DATA_DIR manualmente.\")\n",
    "\n",
    "DATA_DIR = DATA_DIR.resolve()\n",
    "DATA_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da767ce-ec29-4049-95db-4616bc4dc02a",
   "metadata": {},
   "source": [
    "### Lectores y utilidades\n",
    "\n",
    "Este bloque define **funciones de lectura y limpieza** para construir dataframes consistentes a partir de los archivos del MIT-BIH (CSV de señales y TXT de anotaciones). Deja todo listo para el EDA y para funciones posteriores (p. ej., `build_beats_dataframe`).\n",
    "\n",
    "- **_clean_signal_df(df)**: estandariza columnas (`sample`, `MLII`, `V1`–`V6`), convierte a numérico, quita filas vacías, asegura `sample` (int) y crea `time_sec = sample / FS`. **Error** si falta `sample`. → **Devuelve señales limpias**.\n",
    "\n",
    "- **read_signal_csv(path)**: `read_csv` + `_clean_signal_df`. → **DataFrame de señales listo**.\n",
    "\n",
    "- **read_annotations_txt(path)**: lee TXT (FWF o whitespace), asegura `sample` (int) y `type` (category), calcula `time_sec` desde `Time` o `sample/FS`, completa `Sub/Chan/Num/Aux` si faltan. → **`['sample','type','Sub','Chan','Num','Aux','time_sec']`**.\n",
    "\n",
    "- **list_record_ids(base_dir, min_records=1)**: lista ordenada de IDs a partir de archivos `NNN.csv`; **error** si hay menos de `min_records`.\n",
    "\n",
    "- **load_record(record_id, base_dir)**: carga **señales** y **anotaciones**, añade `record`, y hace *left join* de `type` (anotaciones) sobre señales por `sample`. → **Retorna `(sig, ann)`**.\n",
    "\n",
    "**Supuestos**: `FS` (frecuencia de muestreo) y `DATA_DIR` definidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a196260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lectores y utilidades\n",
    "def _clean_signal_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # normaliza nombres de columnas y tipos para CSV de señales\n",
    "    df = df.copy()\n",
    "    df.columns = [c.replace(\"'\", \"\").strip() for c in df.columns]\n",
    "    rename_map = {}\n",
    "    for c in df.columns:\n",
    "        cc = c.lower().replace(\" \", \"\").replace(\"#\", \"\")\n",
    "        if cc == \"sample\":\n",
    "            rename_map[c] = \"sample\"\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    for c in list(df.columns):\n",
    "        clean = c.strip().upper().replace(\" \", \"\")\n",
    "        if clean in (\"MLII\", \"ML2\"):\n",
    "            df = df.rename(columns={c: \"MLII\"})\n",
    "        elif clean in (\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"):\n",
    "            df = df.rename(columns={c: clean})\n",
    "    if \"sample\" not in df.columns:\n",
    "        raise RuntimeError(\"El archivo de señales no contiene 'sample'.\")\n",
    "    for c in df.columns:\n",
    "        if c != \"type\":\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    canal_cols = [col for col in df.columns if col in (\"MLII\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\")]\n",
    "    df = df.dropna(subset=[\"sample\"], how=\"any\")\n",
    "    if canal_cols:\n",
    "        df = df.dropna(subset=canal_cols, how=\"all\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['sample'] = df['sample'].astype(int)\n",
    "    df['time_sec'] = df['sample'] / FS\n",
    "    if 'type' in df.columns:\n",
    "        df['type'] = df['type'].astype(str).str.strip().replace('', np.nan).astype('category')\n",
    "    return df\n",
    "\n",
    "def read_signal_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return _clean_signal_df(df)\n",
    "\n",
    "def read_annotations_txt(ann_path: Path) -> pd.DataFrame:\n",
    "    colnames = ['Time','sample','type','Sub','Chan','Num','Aux']\n",
    "    try:\n",
    "        df = pd.read_fwf(ann_path, names=colnames, header=0)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(ann_path, sep=r\"\\\\s+\", engine='python', names=colnames, header=0)\n",
    "    df['sample'] = pd.to_numeric(df['sample'], errors='coerce')\n",
    "    df = df.dropna(subset=['sample']).reset_index(drop=True)\n",
    "    df['sample'] = df['sample'].astype(int)\n",
    "    df['type'] = df['type'].astype(str).str.strip()\n",
    "    df = df[df['type'] != \"\"].reset_index(drop=True)\n",
    "    df['type'] = df['type'].astype('category')\n",
    "    try:\n",
    "        pd.to_timedelta(df['Time'])\n",
    "        df['time_sec'] = pd.to_timedelta(df['Time']).dt.total_seconds()\n",
    "    except Exception:\n",
    "        df['time_sec'] = df['sample'] / FS\n",
    "    for col in ['Sub','Chan','Num','Aux']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    return df[['sample','type','Sub','Chan','Num','Aux','time_sec']].copy()\n",
    "\n",
    "def list_record_ids(base_dir: Path = DATA_DIR, min_records: int = 1) -> List[str]:\n",
    "    ids = sorted(p.stem for p in base_dir.glob(\"[0-9][0-9][0-9].csv\"))\n",
    "    if len(ids) < min_records:\n",
    "        raise RuntimeError(f\"Se encontraron {len(ids)} registros en {base_dir}.\")\n",
    "    return ids\n",
    "\n",
    "def load_record(record_id: str, base_dir: Path = DATA_DIR) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    csv_path = base_dir / f\"{record_id}.csv\"\n",
    "    ann_path = base_dir / f\"{record_id}annotations.txt\"\n",
    "    sig = read_signal_csv(csv_path)\n",
    "    ann = read_annotations_txt(ann_path)\n",
    "    sig['record'] = str(record_id)\n",
    "    ann['record'] = str(record_id)\n",
    "    sig = sig.merge(ann[['sample','type']], on='sample', how='left')\n",
    "    sig['type'] = pd.Categorical(sig['type'])\n",
    "    return sig, ann\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bac4e7",
   "metadata": {},
   "source": [
    "### Estandarización a AAMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f3dd6-edaa-4722-975f-b6c3bbca28e9",
   "metadata": {},
   "source": [
    "### Etiquetado AAMI: mapeo de símbolos a 5 clases\n",
    "\n",
    "**Qué hace**\n",
    "- Define `AAMI_MAP` para convertir símbolos de latido de MIT-BIH a las **5 clases AAMI**:  \n",
    "  **N** (normal), **S** (supraventricular), **V** (ventricular), **F** (fusión), **Q** (otros/ruido).\n",
    "- `to_aami(beat_symbol)`: limpia el símbolo y devuelve su clase AAMI.  \n",
    "  Si no está en el mapa o es `NaN`, retorna **'Q'** (u `NaN` si la entrada es `NaN`).\n",
    "- `add_aami_labels(df_ann)`: crea una copia de anotaciones y añade la columna **`aami`** (tipo `category`).\n",
    "\n",
    "**Por qué**\n",
    "- Unifica la gran variedad de **símbolos** de MIT-BIH en un **esquema estándar (AAMI)** para EDA y modelado (clases coherentes y comparables).\n",
    "\n",
    "**Detalle del mapa (ejemplos)**\n",
    "- **N**: `N, L, R, e, j`\n",
    "- **S**: `A, a, J, S`\n",
    "- **V**: `V, E`\n",
    "- **F**: `F`\n",
    "- **Q** (otros/ruido): `P, f, Q, ?, /, ~, |, x` **y cualquier símbolo no mapeado**\n",
    "\n",
    "**Demostración rápida**\n",
    "- `ids = list_record_ids(DATA_DIR)` → lista registros disponibles.\n",
    "- `sig_0, ann_0 = load_record(ids[0])` → carga señales/anotaciones del primer registro.\n",
    "- `ann_0 = add_aami_labels(ann_0)` → agrega columna `aami` lista para análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93581597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AAMI_MAP = {\n",
    "    'N':'N','L':'N','R':'N','e':'N','j':'N',        # N\n",
    "    'A':'S','a':'S','J':'S','S':'S',                # S\n",
    "    'V':'V','E':'V',                                # V\n",
    "    'F':'F',                                        # F\n",
    "    'P':'Q','f':'Q','Q':'Q','?':'Q','/':'Q','~':'Q','|':'Q','x':'Q'  # Q/otros\n",
    "}\n",
    "def to_aami(beat_symbol: str) -> str:\n",
    "    if pd.isna(beat_symbol):\n",
    "        return np.nan\n",
    "    b = str(beat_symbol).strip()\n",
    "    return AAMI_MAP.get(b, 'Q')\n",
    "\n",
    "def add_aami_labels(df_ann: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_ann.copy()\n",
    "    df['aami'] = df['type'].astype(str).map(to_aami).astype('category')\n",
    "    return df\n",
    "\n",
    "ids = list_record_ids(DATA_DIR)\n",
    "print(\"Registros detectados:\", ids[:10], \"… total:\", len(ids))\n",
    "\n",
    "sig_0, ann_0 = load_record(ids[0])\n",
    "ann_0 = add_aami_labels(ann_0)\n",
    "display(ann_0.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcafa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Señal:\", sig_0.shape, \"| columnas:\", sig_0.columns.tolist())\n",
    "print(\"Anotaciones:\", ann_0.shape, \"| columnas:\", ann_0.columns.tolist())\n",
    "\n",
    "display(sig_0.head(3)); display(sig_0.tail(3))\n",
    "display(ann_0.head(10))\n",
    "\n",
    "print(\"\\nInfo de señales:\")\n",
    "display(sig_0.info())\n",
    "\n",
    "print(\"\\nValores faltantes (señal):\")\n",
    "display(sig_0.isna().sum())\n",
    "\n",
    "print(\"\\nResumen estadístico (señal):\")\n",
    "display(sig_0.describe().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd55c50",
   "metadata": {},
   "source": [
    "### Dataset latido-a-latido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9986c-0962-4aa8-86fb-a23d464b25b5",
   "metadata": {},
   "source": [
    "### Dataset latido-a-latido (`build_beats_dataframe`)\n",
    "\n",
    "**Qué es:** Convierte las señales + anotaciones del MIT-BIH en una **tabla donde cada fila es un latido** con su clase **AAMI** y features simples.\n",
    "\n",
    "**Cómo lo hace:** Para cada anotación (pico R) toma:\n",
    "- el **valor puntual** de cada canal (`*_val`) en ese instante, y\n",
    "- la **media** y **desviación estándar** en una **ventana centrada** de `win_ms` ms (`*_mean`, `*_std`).\n",
    "Además guarda `record`, `sample`, `time_sec`, `type`, `aami`.\n",
    "\n",
    "**Para qué sirve:** Deja los datos listos para **EDA** (distribuciones, correlaciones, outliers, desbalance) y **modelado** rápido (baselines tabulares o recortes para espectrogramas/CNN).\n",
    "\n",
    "> Nota: si el latido está muy cerca del inicio/fin, la ventana puede quedar incompleta y algunas features serán `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afacfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_beats_dataframe(record_ids: List[str], base_dir: Path = DATA_DIR, win_ms: int = 100) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    half_win = int((win_ms/1000.0) * FS / 2)\n",
    "    for rid in record_ids:\n",
    "        sig, ann = load_record(rid, base_dir)\n",
    "        ann = add_aami_labels(ann)\n",
    "        channel_cols = [c for c in sig.columns if c in (\"MLII\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\")]\n",
    "        sig_idx = sig.set_index('sample')\n",
    "        for _, a in ann.iterrows():\n",
    "            s = int(a['sample'])\n",
    "            rec = {'record': rid, 'sample': s, 'time_sec': a['time_sec'],\n",
    "                   'type': str(a['type']), 'aami': str(a['aami'])}\n",
    "            for ch in channel_cols:\n",
    "                rec[f'{ch}_val'] = sig_idx.loc[s, ch] if s in sig_idx.index else np.nan\n",
    "            lo, hi = s - half_win, s + half_win\n",
    "            win = sig_idx.loc[lo:hi, channel_cols] if (lo in sig_idx.index and hi in sig_idx.index) else None\n",
    "            if win is not None and len(win) > 1:\n",
    "                for ch in channel_cols:\n",
    "                    rec[f'{ch}_mean'] = float(win[ch].mean())\n",
    "                    rec[f'{ch}_std']  = float(win[ch].std())\n",
    "            rows.append(rec)\n",
    "    dfb = pd.DataFrame(rows)\n",
    "    for c in ['record','type','aami']:\n",
    "        if c in dfb.columns:\n",
    "            dfb[c] = dfb[c].astype('category')\n",
    "    return dfb\n",
    "\n",
    "n_ids = min(10, len(ids))\n",
    "beats_df = build_beats_dataframe(ids[:n_ids])\n",
    "print(beats_df.shape)\n",
    "display(beats_df.head())\n",
    "display(beats_df['aami'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b67917-ab8f-4921-9337-1ed7a7d99bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"\\nInfo anotaciones:\")\n",
    "    ann_0.info()\n",
    "    print(\"\\nNA anotaciones:\")\n",
    "    print(ann_0.isna().sum())\n",
    "except Exception as e:\n",
    "    print(\"No se pudo evaluar ann_0:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nInfo beats_df:\")\n",
    "    beats_df.info()\n",
    "    print(\"\\nNA beats_df:\")\n",
    "    print(beats_df.isna().sum())\n",
    "    print(\"\\nTail beats_df:\")\n",
    "    print(beats_df.tail())\n",
    "except Exception as e:\n",
    "    print(\"No se pudo evaluar beats_df:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87e89a",
   "metadata": {},
   "source": [
    "## 1.2) Análisis Univariado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657444f6-f697-46f9-a8e5-c2ddab6226bb",
   "metadata": {},
   "source": [
    "### Análisis univariado — numéricas y clase objetivo\n",
    "\n",
    "- **Selección de features numéricas**\n",
    "  - `num_cols = [...]` toma columnas con sufijos `*_val`, `*_mean`, `*_std` (features por latido).\n",
    "- **Estadísticos básicos + variabilidad**\n",
    "  - `describe().T` da conteo, media, std, qtiles, etc.\n",
    "  - `coef_var = std / mean` (usa `NaN` si `mean==0`) para comparar **variabilidad relativa** entre features.\n",
    "\n",
    "- **Distribución de una variable ejemplo**\n",
    "  - `col_example`: primera columna `*_val` disponible (o la primera numérica).\n",
    "  - **Histograma** (bins=50) → forma de la distribución.\n",
    "  - **Boxplot** → mediana, IQR y colas (posibles outliers).\n",
    "\n",
    "- **Outliers estadísticos (Z-score)**\n",
    "  - `zscore(..., nan_policy='omit')` sobre **todas** las numéricas.\n",
    "  - `outlier_mask = (z > 3).any(axis=1)` cuenta latidos con al menos un feature fuera de ±3σ.\n",
    "\n",
    "- **Distribución de clases (variable objetivo `aami`)**\n",
    "  - `value_counts()` y **barras de proporción** para ver **desbalance** entre clases AAMI.\n",
    "\n",
    "> Ajustes comunes: cambiar `bins`, umbral de outliers (p. ej. 3.5σ), o filtrar/transformar (log) antes de graficar si hay colas muy largas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ad3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = [c for c in beats_df.columns if any(s in c for s in ['_val','_mean','_std'])]\n",
    "desc = beats_df[num_cols].describe().T\n",
    "desc['coef_var'] = desc['std'] / desc['mean'].replace(0, np.nan)\n",
    "display(desc.head(20))\n",
    "\n",
    "col_example = next((c for c in num_cols if c.endswith('_val')), num_cols[0])\n",
    "fig, ax = plt.subplots(); ax.hist(beats_df[col_example].dropna(), bins=50)\n",
    "ax.set_title(f'Histograma: {col_example}'); ax.set_xlabel(col_example); ax.set_ylabel('Frecuencia'); plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(); ax.boxplot(beats_df[col_example].dropna(), vert=True)\n",
    "ax.set_title(f'Boxplot: {col_example}'); ax.set_ylabel(col_example); plt.show()\n",
    "\n",
    "z = np.abs(stats.zscore(beats_df[num_cols].dropna(), nan_policy='omit'))\n",
    "outlier_mask = (z > 3).any(axis=1)\n",
    "print(\"Outliers (Z>3):\", int(outlier_mask.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_counts = beats_df['aami'].value_counts().sort_values(ascending=False)\n",
    "display(cat_counts)\n",
    "(cat_counts / cat_counts.sum()).plot(kind='bar', title='Distribución de clases AAMI')\n",
    "plt.xlabel('Clase AAMI'); plt.ylabel('Proporción'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617d478-3f61-46ef-80d5-8e828083e123",
   "metadata": {},
   "source": [
    "- Variables numéricas: histogramas y boxplots de '*_val*' (p. ej., MLII_val, V5_val).\n",
    "- Estadísticas de distribución: media/mediana/moda (desde describe y visualizaciones).\n",
    "-  Outliers: visualizados en boxplots; consistentes con artefactos/latidos ectópicos.\n",
    "-  Variables categóricas: barras de frecuencia/proporción por clase AAMI (N, S, V, F, Q).\n",
    "Hallazgos: distribuciones unimodales con colas; presencia de outliers; desbalance marcado (N≫S,V; F,Q marginales).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86106b64",
   "metadata": {},
   "source": [
    "## 1.3 Análisis Bivariado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7723d18-0fcc-46fe-b029-f8946388ba92",
   "metadata": {},
   "source": [
    "### Análisis bivariado — correlaciones, dispersión y clase vs valor\n",
    "\n",
    "- **Selección de columnas crudas (`*_val`)**\n",
    "  - `val_cols = [...]` toma solo las features puntuales por canal (no medias/std).\n",
    "  - Se exige ≥2 columnas para poder correlacionar y graficar `scatter`.\n",
    "\n",
    "- **Preparación**\n",
    "  - `X = beats_df[val_cols].dropna(how='all')`: descarta filas sin datos en **todas** las `*_val`.\n",
    "\n",
    "- **Heatmap de correlación (Pearson)**\n",
    "  - `corr = X.corr(method='pearson')` → asume relación **lineal** y sensibilidad a **outliers**.\n",
    "  - `sns.heatmap(corr, ...)` muestra fuerza/dirección de relación entre canales `*_val`.\n",
    "  - Útil para detectar **redundancia** y seleccionar features.\n",
    "\n",
    "- **Dispersión entre dos variables**\n",
    "  - `plt.scatter(beats_df[xcol], beats_df[ycol], alpha=0.3)` visualiza relación (forma/nube, colas, clusters).\n",
    "  - Sirve para ver **linealidad** y posibles **outliers** bivariados.\n",
    "\n",
    "- **Boxplot por clase AAMI**\n",
    "  - Para una `col_example` (`*_val`), compara su distribución **entre clases** (`aami`).\n",
    "  - Ayuda a evaluar **separabilidad** de la variable respecto a la **clase objetivo**.\n",
    "\n",
    "> Notas:\n",
    "> - Si hay no linealidad o muchos outliers, prueba `corr(method='spearman')`.\n",
    "> - Considera estandarizar o recortar extremos antes de scatter si hay escalas muy distintas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51262a5e-f086-4448-8d25-22398aa65081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Solo columnas de valor crudo ===\n",
    "val_cols = [c for c in beats_df.columns if c.endswith('_val')]\n",
    "if len(val_cols) < 2:\n",
    "    raise ValueError(\"No hay suficientes columnas *_val para correlación/scatter.\")\n",
    "\n",
    "# Quita filas totalmente vacías en esas columnas\n",
    "X = beats_df[val_cols].dropna(how='all')\n",
    "\n",
    "# --- Heatmap de correlación (Pearson) SOLO con *_val ---\n",
    "corr = X.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0, annot=False)\n",
    "plt.title('Matriz de correlación (solo *_val)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Scatter entre dos columnas *_val (ejemplo) ---\n",
    "xcol, ycol = val_cols[0], val_cols[1]\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(beats_df[xcol], beats_df[ycol], alpha=0.3)\n",
    "plt.title(f'{xcol} vs {ycol}')\n",
    "plt.xlabel(xcol); plt.ylabel(ycol)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Boxplot de una columna *_val por clase AAMI (sin estadísticas) ---\n",
    "col_example = val_cols[0]\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "cats = beats_df['aami'].cat.categories if hasattr(beats_df['aami'], 'cat') else sorted(beats_df['aami'].dropna().unique())\n",
    "groups = [beats_df.loc[beats_df['aami'] == k, col_example].dropna() for k in cats]\n",
    "ax.boxplot(groups, labels=list(cats))\n",
    "ax.set_title(f'{col_example} por clase AAMI')\n",
    "ax.set_xlabel('Clase AAMI'); ax.set_ylabel(col_example)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0188c-b3fe-497f-a60c-3f73d276d205",
   "metadata": {},
   "source": [
    "- Matriz de correlación (Pearson) usando solo columnas '*_val*'.\n",
    "- Scatter plots entre variables '*_val*' para evaluar relaciones y posible separación por clase (recomendado colorear por 'aami').\n",
    "- Boxplots de '*_val*' por clase AAMI para observar diferencias de medianas/variabilidad.\n",
    "- Análisis con variable objetivo: diferencias por clase observables visualmente.\n",
    "Hallazgos: correlaciones bajas–moderadas (MLII vs V5) sin multicolinealidad; diferencias por clase visibles en boxplots (V suele mostrar mayor variabilidad).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c7457",
   "metadata": {},
   "source": [
    "## 1.4 Análisis multivariado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f55ff-5678-4572-877a-237cd4de28e4",
   "metadata": {},
   "source": [
    "### Análisis multivariado — dispersión base y cómo escalarlo\n",
    "\n",
    "**Qué hace este bloque**\n",
    "- Toma dos columnas `*_val` (`x`, `y`) y dibuja un **scatter** (`alpha=0.3`, `s=10`) para ver la relación entre **dos** variables.\n",
    "- Nota: con solo `x` e `y` es técnicamente **bivariado**; sirve como base para multivariado.\n",
    "\n",
    "**Cómo volverlo realmente multivariado (3+ variables)**\n",
    "- **Color** por `aami` (tercera variable) y/o **tamaño** por alguna `*_std` (cuarta variable):\n",
    "```python\n",
    "cats = beats_df['aami'].astype('category')\n",
    "c = cats.cat.codes  # color por clase AAMI\n",
    "s = 10 * (1 + beats_df.filter(like='_std').iloc[:,0].fillna(0).rank(pct=True))  # tamaño por variabilidad\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(beats_df[x], beats_df[y], c=c, s=s, alpha=0.35)\n",
    "plt.title(f'{x} vs {y} (color: AAMI, tamaño: std)')\n",
    "plt.xlabel(x); plt.ylabel(y); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8577ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo *_val para evitar medias/std\n",
    "val_cols = [c for c in beats_df.columns if c.endswith('_val')]\n",
    "x, y = val_cols[0], val_cols[1]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(beats_df[x], beats_df[y], alpha=0.3, s=10)\n",
    "plt.title(f'{x} vs {y}')\n",
    "plt.xlabel(x); plt.ylabel(y)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871aa801",
   "metadata": {},
   "source": [
    "## 1.5 Detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a8adb-0ea4-4018-a622-bc9a761195a7",
   "metadata": {},
   "source": [
    "### Detección de anomalías — patrones y calidad de datos\n",
    "\n",
    "### 1) Distribución de clases (AAMI)\n",
    "- Grafica los **conteos por clase** (`aami`).  \n",
    "- Sirve para **detectar desbalance** (p. ej., clases muy raras) que afecta el modelado y la evaluación.\n",
    "\n",
    "### 2) Patrones temporales (bins de 10 s)\n",
    "- Normaliza el tiempo por `record` (`t_rel_s`) y agrupa en **ventanas de 10 s** (`bin_10s`).\n",
    "- Para un `record` ejemplo, grafica **conteos por clase en el tiempo**.  \n",
    "- Útil para ver **ráfagas**/episodios anómalos (picos de V, S, etc.) y **estacionaridad**.\n",
    "\n",
    "### 3) Outliers por IQR (boxplot de una `*_val`)\n",
    "- Boxplot de una feature cruda (`col_example`) para **IQR y puntos extremos**.  \n",
    "- Ayuda a detectar **outliers univariados** y distribuciones con **colas largas**.\n",
    "\n",
    "### 4) Chequeos programáticos de calidad (data sanity)\n",
    "- **Duplicados**: `sig_0.duplicated()` y `beats_df.duplicated()` → registros repetidos.\n",
    "- **Rangos extremos**: cuantiles `[0.1%, 99.9%]` en `sig_0` → valores fuera de rango esperado.\n",
    "- **Monotonicidad de `sample`**: debe **crecer** (muestras en orden temporal).\n",
    "- **Anotaciones fuera de rango**: `ann_0['sample']` debe caer dentro del rango de `sig_0['sample']`.\n",
    "\n",
    "> Conjunto: (1–3) detectan **anomalías de comportamiento**; (4) detecta **inconsistencias del dataset**. Ambos son necesarios antes de modelar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d294c0b",
   "metadata": {},
   "source": [
    "## Patch · Cuantificación de outliers por feature (IQR)\n",
    "Se listan umbrales **IQR (1.5×IQR)** por variable numérica y el **conteo de outliers** para priorizar limpieza/caps.\n",
    "> **Configurable:** ajusta `DF_CANDIDATES` si tu `DataFrame` principal tiene otro nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e93f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DF_CANDIDATES = ['beats_df','df','data','dataset']\n",
    "\n",
    "def _pick_df():\n",
    "    g = globals()\n",
    "    for name in DF_CANDIDATES:\n",
    "        if name in g and isinstance(g[name], pd.DataFrame):\n",
    "            return g[name], name\n",
    "    raise ValueError(\"No se pudo detectar automáticamente el DataFrame. Ajusta DF_CANDIDATES.\")\n",
    "\n",
    "df, df_name = _pick_df()\n",
    "\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "rows = []\n",
    "for col in num_cols:\n",
    "    s = df[col].dropna()\n",
    "    if s.empty:\n",
    "        continue\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    n_out = int(((s < lo) | (s > hi)).sum())\n",
    "    rows.append((col, float(lo), float(hi), n_out))\n",
    "\n",
    "out_df = pd.DataFrame(rows, columns=['feature','low','high','n_outliers']).sort_values('n_outliers', ascending=False)\n",
    "print(f\"DF detectado: {df_name}\")\n",
    "display(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b13c7-2d70-4fff-92e1-417c95c341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1) Distribución por clase (AAMI)\n",
    "# ============================\n",
    "ax = beats_df['aami'].value_counts().sort_index().plot(kind='bar', figsize=(6,4))\n",
    "ax.set_title('Distribución de latidos por clase AAMI')\n",
    "ax.set_xlabel('Clase AAMI'); ax.set_ylabel('Conteos')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2) Conteos cada 10s (un solo registro), leyenda = clases\n",
    "# ============================\n",
    "df = beats_df.copy()\n",
    "\n",
    "# tiempo relativo por registro (arranca en 0 s)\n",
    "df['t_rel_s'] = df.groupby('record')['time_sec'].transform(lambda s: s - s.min())\n",
    "\n",
    "# bin de 10s (0, 10, 20, ...)\n",
    "df['bin_10s'] = (df['t_rel_s'] // 10).astype(int) * 10\n",
    "\n",
    "# elige un registro (el primero)\n",
    "rid = str(df['record'].astype(str).iloc[0])\n",
    "\n",
    "# tabla de conteos por bin y clase\n",
    "cur = (df[df['record'].astype(str) == rid]\n",
    "       .groupby(['bin_10s','aami']).size()\n",
    "       .rename('count').reset_index())\n",
    "\n",
    "# pivot para graficar: filas = tiempo (seg), columnas = clase\n",
    "plot_df = cur.pivot(index='bin_10s', columns='aami', values='count').fillna(0)\n",
    "\n",
    "ax = plot_df.plot(figsize=(10,4))\n",
    "ax.set_title(f'Conteos cada 10s por clase — record {rid}')\n",
    "ax.set_xlabel('Tiempo (segundos)'); ax.set_ylabel('Conteos')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ebe63-83b3-4a97-8cca-6bac6d92ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cols = [c for c in beats_df.columns if c.endswith('_val')]\n",
    "col_example = val_cols[0]\n",
    "beats_df[[col_example]].boxplot(figsize=(5,4))\n",
    "plt.title(f'Boxplot de {col_example} (IQR y outliers)')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c4ea6-1db5-46ce-976a-118699b4adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Duplicados\n",
    "try:\n",
    "    d_sig = int(sig_0.duplicated().sum())\n",
    "    print(\"Duplicados en señales:\", d_sig)\n",
    "except Exception as e:\n",
    "    print(\"Duplicados en señales: N/A\", e)\n",
    "\n",
    "try:\n",
    "    d_beats = int(beats_df.duplicated().sum())\n",
    "    print(\"Duplicados en beats_df:\", d_beats)\n",
    "except Exception as e:\n",
    "    print(\"Duplicados en beats_df: N/A\", e)\n",
    "\n",
    "# Rangos por cuantiles extremos (sanity checks)\n",
    "try:\n",
    "    num_cols_sig = sig_0.select_dtypes(include=[np.number]).columns\n",
    "    if len(num_cols_sig) > 0:\n",
    "        q = sig_0[num_cols_sig].quantile([0.001, 0.999])\n",
    "        print(\"\\nQuantiles [0.1%, 99.9%] señales:\")\n",
    "        print(q)\n",
    "    else:\n",
    "        print(\"No hay columnas numéricas en sig_0.\")\n",
    "except Exception as e:\n",
    "    print(\"Quantiles señales: N/A\", e)\n",
    "\n",
    "# Monotonicidad de sample\n",
    "try:\n",
    "    monotonic = bool(sig_0['sample'].is_monotonic_increasing)\n",
    "    print(\"\\nsample monotónico en señales:\", monotonic)\n",
    "except Exception as e:\n",
    "    print(\"Monotonicidad sample: N/A\", e)\n",
    "\n",
    "# Anotaciones dentro del rango de sample\n",
    "try:\n",
    "    ann_outside = (~ann_0['sample'].between(sig_0['sample'].min(), sig_0['sample'].max())).sum()\n",
    "    print(\"Anotaciones fuera de rango de sample:\", int(ann_outside))\n",
    "except Exception as e:\n",
    "    print(\"Chequeo de rango de anotaciones: N/A\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74f5b8-5c0a-4a9b-8e58-979ac56443d3",
   "metadata": {},
   "source": [
    "- Detección de outliers: lectura visual mediante boxplots; opcionalmente IQR para conteo.\n",
    "- Patrones temporales: conteos por bins de 10 s con tiempo relativo por 'record' y columnas = clases AAMI.\n",
    "- Distribuciones anómalas/inconsistencias: revisión de colas y segmentos con actividad S/V intensa.\n",
    "Hallazgos: episodios temporales con picos de S/V; recomendable muestreo estratificado por segmentos activos para balancear ventanas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca925c-3342-450f-b1ba-ff382d5d3cea",
   "metadata": {},
   "source": [
    "### Variable objetivo (AAMI) — distribución, desbalance y relación con features\n",
    "\n",
    "- **Clasificación**: se grafica la distribución de clases `aami`, se calculan métricas de **desbalance** (IR, entropía, Gini) y **pesos sugeridos**.\n",
    "- **Relación con predictoras**: ANOVA por feature (`*_val/_mean/_std`) + tamaño de efecto (η²) y boxplots de las más discriminativas.\n",
    "- **Regresión (opcional)**: si hubiera un objetivo numérico (`target`, `y`, `target_reg`), se grafica su distribución y correlación con las features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a689dfa",
   "metadata": {},
   "source": [
    "## Patch · Análisis explícito de desbalance de clases\n",
    "Cálculo de métricas de desbalance: **Imbalance Ratio (IR)** por clase, **entropía**, **índice de Gini** y **class_weight** sugeridos para entrenamiento.\n",
    "> **Configurable:** el código intenta detectar automáticamente el `DataFrame` y la columna de etiqueta. Si no coincide con tus nombres, edita `DF_CANDIDATES` y `LABEL_CANDIDATES` al inicio del bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b988c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intento de autoconfiguración: escoger DF y etiqueta objetivo (AAMI/label)\n",
    "DF_CANDIDATES = ['beats_df','df','data','dataset']\n",
    "LABEL_CANDIDATES = ['aami','AAMI','label','class','target','y']\n",
    "\n",
    "def _pick_df_and_label():\n",
    "    import pandas as pd\n",
    "    g = globals()\n",
    "    for df_name in DF_CANDIDATES:\n",
    "        if df_name in g and isinstance(g[df_name], pd.DataFrame):\n",
    "            df = g[df_name]\n",
    "            for col in LABEL_CANDIDATES:\n",
    "                if col in df.columns:\n",
    "                    return df, col, df_name\n",
    "    raise ValueError(\"No se pudo detectar automáticamente el DF/columna. Ajusta DF_CANDIDATES/LABEL_CANDIDATES.\")\n",
    "\n",
    "try:\n",
    "    df, label_col, df_name = _pick_df_and_label()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y = df[label_col].dropna()\n",
    "cnt = Counter(y)\n",
    "N = sum(cnt.values())\n",
    "props = {k: v/N for k, v in cnt.items()}\n",
    "maj = max(cnt.values())\n",
    "IR = {k: maj/v for k, v in cnt.items()}\n",
    "p = np.array(list(props.values()), dtype=float)\n",
    "entropy = float(-(p * np.log(p + 1e-12)).sum())\n",
    "gini = float(1 - (p**2).sum())\n",
    "class_weight = {k: float(N / (len(cnt) * v)) for k, v in cnt.items()}\n",
    "\n",
    "res = pd.DataFrame({\n",
    "    'count': pd.Series(cnt, dtype='float'),\n",
    "    'prop': pd.Series(props, dtype='float'),\n",
    "    'IR': pd.Series(IR, dtype='float'),\n",
    "    'class_weight': pd.Series(class_weight, dtype='float'),\n",
    "}).rename_axis('clase').sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"DF detectado: {df_name} | etiqueta: {label_col}\")\n",
    "display(res)\n",
    "print(f\"Entropía={entropy:.3f}  |  Gini={gini:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5f000-4aa7-47b5-90da-1198aa0f6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Config ====\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "target_cls = 'aami'  # objetivo para clasificación (AAMI)\n",
    "\n",
    "# ==== 1) Distribución de clases (clasificación) ====\n",
    "assert target_cls in beats_df.columns, f\"No existe la columna objetivo '{target_cls}'.\"\n",
    "cls_counts = beats_df[target_cls].value_counts(dropna=False).sort_index()\n",
    "cls_props  = cls_counts / cls_counts.sum()\n",
    "\n",
    "display(pd.DataFrame({'count': cls_counts, 'prop': cls_props}).rename_axis('clase'))\n",
    "\n",
    "ax = cls_props.plot(kind='bar', rot=0, figsize=(6,4))\n",
    "ax.set_title('Distribución de clases (AAMI)')\n",
    "ax.set_xlabel('Clase'); ax.set_ylabel('Proporción')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ==== 2) Métricas de desbalance ====\n",
    "# IR (Imbalance Ratio) = mayoria/minoria\n",
    "ir = (cls_counts.max() / cls_counts[cls_counts > 0].min()) if (cls_counts > 0).any() else np.nan\n",
    "entropy_bits = -np.sum(cls_props.values * np.log2(np.clip(cls_props.values, 1e-12, 1)))\n",
    "gini_impurity = 1 - np.sum(cls_props.values**2)\n",
    "\n",
    "# Pesos balanceados (útiles para sklearn)\n",
    "class_weights = {str(k): (len(beats_df) / (len(cls_counts) * v)) for k, v in cls_counts.items() if v > 0}\n",
    "\n",
    "print(f\"IR (mayoría/minoría): {ir:.2f}\")\n",
    "print(f\"Entropía (bits): {entropy_bits:.3f}\")\n",
    "print(f\"Gini impurity: {gini_impurity:.3f}\")\n",
    "print(\"Pesos sugeridos (class_weight):\", class_weights)\n",
    "\n",
    "# ==== 3) Relación con variables predictoras ====\n",
    "num_cols = [c for c in beats_df.columns if any(s in c for s in ['_val','_mean','_std'])]\n",
    "anova_rows = []\n",
    "\n",
    "for col in num_cols:\n",
    "    # grupos por clase (solo si hay datos)\n",
    "    groups = [beats_df.loc[beats_df[target_cls]==k, col].dropna() for k in cls_counts.index]\n",
    "    valid = [g for g in groups if len(g) >= 3]\n",
    "    if len(valid) < 2:\n",
    "        anova_rows.append((col, np.nan, np.nan, np.nan))\n",
    "        continue\n",
    "    # ANOVA de una vía\n",
    "    try:\n",
    "        F, p = stats.f_oneway(*valid)\n",
    "    except Exception:\n",
    "        F, p = np.nan, np.nan\n",
    "    # Tamaño de efecto (eta^2)\n",
    "    all_vals = pd.concat(valid)\n",
    "    grand = all_vals.mean()\n",
    "    ssb = sum(len(g)*(g.mean()-grand)**2 for g in valid)\n",
    "    sst = sum(((g - grand)**2).sum() for g in valid)\n",
    "    eta2 = ssb/sst if sst > 0 else np.nan\n",
    "    anova_rows.append((col, F, p, eta2))\n",
    "\n",
    "anova_df = (pd.DataFrame(anova_rows, columns=['feature','F','p_value','eta2'])\n",
    "            .sort_values(['p_value','eta2'], na_position='last'))\n",
    "display(anova_df.head(15))\n",
    "\n",
    "# Boxplots de las 3 features más discriminativas (menor p-value)\n",
    "topk = [f for f in anova_df['feature'].dropna().head(3)]\n",
    "for col in topk:\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    cats = list(cls_counts.index)\n",
    "    data = [beats_df.loc[beats_df[target_cls]==k, col].dropna() for k in cats]\n",
    "    ax.boxplot(data, labels=[str(k) for k in cats])\n",
    "    ax.set_title(f'{col} por clase AAMI')\n",
    "    ax.set_xlabel('Clase AAMI'); ax.set_ylabel(col)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# ==== 4) (Opcional) Caso regresión: distribución y relación con predictoras ====\n",
    "# Si tuvieras un objetivo numérico, define su nombre aquí o detecta uno común:\n",
    "reg_candidates = [c for c in ['target','y','target_reg'] if c in beats_df.columns and pd.api.types.is_numeric_dtype(beats_df[c])]\n",
    "reg_target = reg_candidates[0] if len(reg_candidates) else None\n",
    "\n",
    "if reg_target:\n",
    "    print(f\"[Regresión] Objetivo detectado: {reg_target}\")\n",
    "    # Distribución del objetivo numérico\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.hist(beats_df[reg_target].dropna(), bins=50)\n",
    "    ax.set_title(f'Distribución del objetivo: {reg_target}')\n",
    "    ax.set_xlabel(reg_target); ax.set_ylabel('Frecuencia')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Correlación de features con el objetivo\n",
    "    corr_with_y = (beats_df[num_cols + [reg_target]]\n",
    "                   .corr(method='pearson')[reg_target]\n",
    "                   .drop(index=reg_target).sort_values(ascending=False))\n",
    "    display(corr_with_y.to_frame('corr_con_objetivo').head(15))\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(beats_df[num_cols + [reg_target]].corr(), cmap='coolwarm', center=0)\n",
    "    plt.title('Correlación features ↔ objetivo (regresión)')\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"No se detectó objetivo numérico para regresión (sección opcional omitida).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb8315",
   "metadata": {},
   "source": [
    "\n",
    "## 1.6 Conclusiones e *insights*\n",
    "## Hallazgos principales\n",
    "\n",
    "- Se identificó un **desbalance severo** en la distribución de clases AAMI: la clase **N** predomina ampliamente, mientras que las clases **S** y **V** aparecen en menor proporción, y **F** y **Q** son marginales.\n",
    "- Las **correlaciones entre canales crudos** (como MLII y V5) fueron **bajas a moderadas**, sin evidencia de colinealidad fuerte, lo que permite su combinación o tratamiento independiente.\n",
    "- Las señales crudas mostraron **distribuciones unimodales con colas largas y presencia de outliers**, lo cual sugiere artefactos, deriva de línea base o eventos fisiológicos atípicos (p. ej., latidos ectópicos).\n",
    "- El análisis temporal por bloques de 10 segundos reveló **episodios localizados de actividad S/V**, evidenciando una distribución **no estacionaria** a lo largo del registro.\n",
    "- Se validó el correcto **mapeo de anotaciones MIT-BIH a clases AAMI** (N, S, V, F, Q), así como la estandarización de la frecuencia de muestreo (360 Hz).\n",
    "\n",
    "\n",
    "## Implicaciones para el modelado (CNN + espectrogramas + XAI)\n",
    "\n",
    "- El **desbalance de clases** obliga a emplear estrategias como **ponderación por clase**, **muestreo estratificado por registro** y enfoques cuidadosos de augmentación.\n",
    "- Se observaron **diferencias en amplitud y variabilidad entre clases** en los valores crudos, lo que justifica una **normalización por ventana** previa a la extracción espectral (STFT/CWT).\n",
    "- La dinámica episódica de S/V sugiere que el muestreo de entrenamiento debe enfocarse en **ventanas representativas** de esas clases para evitar sobreajuste a N.\n",
    "- La baja colinealidad entre MLII y V5 permite la **entrada multi-canal** en modelos CNN, ya sea apilando canales o mediante fusión posterior.\n",
    "- Para la explicación de decisiones (XAI), se propone el uso de **Grad-CAM o Integrated Gradients** sobre espectrogramas log-power, permitiendo **localizar bandas temporales relevantes por clase**.\n",
    "\n",
    "\n",
    "## Recomendaciones de preprocesamiento\n",
    "\n",
    "1. **Filtrado y detrending**: aplicar filtros pasa-banda (0.5–40 Hz), eliminación de ruido de red (50/60 Hz si aplica), y corrección de la línea base.\n",
    "2. **Normalización por ventana**: usar z-score o métodos robustos (mediana/IQR), evitando la normalización global.\n",
    "3. **Clipping robusto**: limitar amplitudes a los percentiles [1, 99] para reducir la influencia de artefactos severos.\n",
    "4. **Ventaneo de señales**: segmentos de 5 segundos con 50% de solape; asignación de etiquetas basada en la clase dominante o presencia de S/V.\n",
    "5. **Control de fuga de información**: asegurar partición por paciente/registro en fases de entrenamiento, validación y prueba.\n",
    "\n",
    "\n",
    "\n",
    "## Próximos pasos\n",
    "\n",
    "1. Implementar el pipeline de **ventaneo con etiquetas AAMI** y exportación de **espectrogramas log-power** por canal.\n",
    "2. Construir un índice estructurado (CSV/Parquet) con **metadatos clave**: ruta, canal, clase AAMI, timestamps y parámetros de transformación espectral.\n",
    "3. Entrenar una arquitectura base **CNN ligera** (e.g., ResNet-18/EfficientNet-B0) con **class weighting y validación por sujeto**, registrando métricas clave como F1-macro y recall por clase.\n",
    "4. Aplicar técnicas de XAI (**Grad-CAM**) sobre predicciones de clases S y V para identificar **activaciones relevantes** y retroalimentar ajustes de preprocesamiento.\n",
    "5. Documentar los resultados mediante **matrices de confusión, curvas PR por clase y análisis de errores**, integrando consideraciones clínicas para una evaluación más completa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84f216-8543-43af-b3df-3c75d9875ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb7dc85",
   "metadata": {},
   "source": [
    "## Patch · Visualización multivariada (3+ variables)\n",
    "**Scatter** de dos variables numéricas, coloreado por la **clase objetivo**; el **tamaño** puede mapearse a una tercera numérica (opcional).\n",
    "> **Configurable:** si no se detecta automáticamente, asigna manualmente `x_col`, `y_col` y `size_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Detectar DF y etiqueta\n",
    "DF_CANDIDATES = ['beats_df','df','data','dataset']\n",
    "LABEL_CANDIDATES = ['aami','AAMI','label','class','target','y']\n",
    "\n",
    "def _pick_df_and_label():\n",
    "    g = globals()\n",
    "    for df_name in DF_CANDIDATES:\n",
    "        if df_name in g and isinstance(g[df_name], pd.DataFrame):\n",
    "            df = g[df_name]\n",
    "            for col in LABEL_CANDIDATES:\n",
    "                if col in df.columns:\n",
    "                    return df, col, df_name\n",
    "    raise ValueError(\"Ajusta DF_CANDIDATES y LABEL_CANDIDATES.\")\n",
    "\n",
    "df, label_col, df_name = _pick_df_and_label()\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != label_col]\n",
    "\n",
    "# Heurística: dos primeras numéricas con varianza mayor\n",
    "if len(num_cols) >= 2:\n",
    "    variances = df[num_cols].var().sort_values(ascending=False)\n",
    "    x_col, y_col = variances.index[:2]\n",
    "else:\n",
    "    raise ValueError(\"No hay suficientes columnas numéricas para scatter.\")\n",
    "\n",
    "size_col = variances.index[2] if len(variances) >= 3 else None\n",
    "\n",
    "plt.figure()\n",
    "for cls, sub in df.groupby(label_col):\n",
    "    sizes = (sub[size_col] - sub[size_col].min()) / (sub[size_col].max() - sub[size_col].min() + 1e-12) * 40 + 10 if size_col else 20\n",
    "    plt.scatter(sub[x_col], sub[y_col], s=sizes, alpha=0.6, label=str(cls))\n",
    "\n",
    "plt.title(f\"Scatter multivariado: {x_col} vs {y_col} (color={label_col}\" + (f\", size={size_col}\" if size_col else \"\") + \")\")\n",
    "plt.xlabel(x_col); plt.ylabel(y_col); plt.legend(title=str(label_col), fontsize=8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
