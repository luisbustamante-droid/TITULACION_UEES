{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c314974",
   "metadata": {},
   "source": [
    "\n",
    "# EDA del MIT‑BIH Arrhythmia Database (PhysioNet) usando `wfdb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78523778",
   "metadata": {},
   "source": [
    "#### Importar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import scipy.stats as st\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "except ImportError:\n",
    "    sm = None\n",
    "    multipletests = None\n",
    "    warnings.warn(\"statsmodels no disponible; algunas pruebas estadísticas no estarán disponibles\")\n",
    "\n",
    "try:\n",
    "    import wfdb\n",
    "except ImportError:\n",
    "    wfdb = None\n",
    "    warnings.warn(\"wfdb no instalado; instala con `pip install wfdb` para poder leer señales crudas\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0fdf9",
   "metadata": {},
   "source": [
    "#### Helpers para lectura de registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_records(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lee el archivo RECORDS en la carpeta del MIT-BIH y devuelve la lista de registros (sin extensión).\n",
    "    Esto permite iterar sobre los registros disponibles.\n",
    "    \"\"\"\n",
    "    recs_path = Path(data_dir) / \"RECORDS\"\n",
    "    if not recs_path.exists():\n",
    "        raise FileNotFoundError(f\"No se encontró el archivo RECORDS en {data_dir}\")\n",
    "    with open(recs_path, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "def load_record_local(record_name: str, data_dir: str) -> Tuple[np.ndarray, dict, wfdb.Annotation]:\n",
    "    \"\"\"\n",
    "    Carga un registro del MIT-BIH desde archivos locales (.dat, .hea, .atr).\n",
    "    Retorna:\n",
    "      - signals: arreglo numpy (n_muestras, n_canales)\n",
    "      - meta: diccionario con metadatos (frecuencia, nombres de señales, longitud)\n",
    "      - annot: objeto wfdb.Annotation con latidos anotados (posiciones, símbolos)\n",
    "    \"\"\"\n",
    "    base_path = str(Path(data_dir) / record_name)\n",
    "    record = wfdb.rdrecord(base_path, physical=True)\n",
    "    signals = record.p_signal\n",
    "    meta = {\n",
    "        \"fs\": record.fs,\n",
    "        \"units\": record.units,\n",
    "        \"sig_name\": record.sig_name,\n",
    "        \"comments\": record.comments,\n",
    "        \"n_sig\": record.n_sig,\n",
    "        \"sig_len\": record.sig_len\n",
    "    }\n",
    "    annot = wfdb.rdann(base_path, extension=\"atr\")\n",
    "    return signals, meta, annot\n",
    "\n",
    "def plot_signal_with_annotations(signals: np.ndarray, annot: wfdb.Annotation, meta: dict, \n",
    "                                 start_sec: float = 0.0, duration_sec: float = 5.0, channel: int = 0):\n",
    "    \"\"\"\n",
    "    Grafica un segmento de la señal ECG con las anotaciones de latidos superpuestas.\n",
    "    - start_sec: segundo inicial\n",
    "    - duration_sec: duración en segundos\n",
    "    - channel: indice del canal a graficar\n",
    "    \"\"\"\n",
    "    fs = meta[\"fs\"]\n",
    "    start_idx = int(start_sec * fs)\n",
    "    end_idx = int((start_sec + duration_sec) * fs)\n",
    "    t = np.arange(start_idx, end_idx) / fs  # tiempo en segundos\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(t, signals[start_idx:end_idx, channel], label=f\"Señal canal {channel}\")\n",
    "    # anotaciones dentro del rango\n",
    "    mask = (annot.sample >= start_idx) & (annot.sample < end_idx)\n",
    "    ann_x = (annot.sample[mask] / fs)\n",
    "    ann_y = signals[annot.sample[mask], channel]\n",
    "    plt.scatter(ann_x, ann_y, c='red', marker='x', label=\"Latidos anotados\")\n",
    "    plt.title(f\"Registro — señal ECG (canal {channel}) con anotaciones\\nSegmento {start_sec:.1f}–{start_sec+duration_sec:.1f} s\")\n",
    "    plt.xlabel(\"Tiempo (s)\")\n",
    "    plt.ylabel(\"Voltaje (mV)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_basic_stats(signals: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula estadísticas simples de la señal completa:\n",
    "    min, max, mean, median, std.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"min\": float(np.min(signals)),\n",
    "        \"max\": float(np.max(signals)),\n",
    "        \"mean\": float(np.mean(signals)),\n",
    "        \"median\": float(np.median(signals)),\n",
    "        \"std\": float(np.std(signals))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52caff59-097a-46f1-8186-f06d86e1ac1b",
   "metadata": {},
   "source": [
    "## EJECUCIÓN del EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec910a-363b-4bf1-ba34-21c0978fe566",
   "metadata": {},
   "source": [
    "### 1.1 Carga y Exploración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789fbda-85af-4c07-a0e5-bebce66f3c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Sección 1.1: Carga y Exploración Inicial\n",
    "# ================================\n",
    "\n",
    "# — Parámetro: ruta al directorio del dataset local\n",
    "DATA_DIR = \"./mit-bih-arrhythmia-database-1.0.0\"  # Ajusta según tu organización de carpetas\n",
    "\n",
    "# — Listar los registros disponibles mediante el helper que definiste\n",
    "records = list_records(DATA_DIR)\n",
    "print(\"Cantidad de registros disponibles:\", len(records))\n",
    "print(\"Algunos registros de ejemplo:\", records[:5])\n",
    "\n",
    "# — Elegir un registro de prueba para exploración inicial\n",
    "rec0 = records[0]\n",
    "\n",
    "# — Cargar señal, metadatos y anotaciones de ese registro\n",
    "signals0, meta0, annot0 = load_record_local(rec0, DATA_DIR)\n",
    "\n",
    "print(f\"\\nMetadatos del registro {rec0}:\")\n",
    "for k, v in meta0.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nPrimeras 10 anotaciones (muestra, símbolo):\")\n",
    "print(list(zip(annot0.sample[:10], annot0.symbol[:10])))\n",
    "\n",
    "# — Visualizar un segmento de la señal con anotaciones superpuestas\n",
    "plot_signal_with_annotations(signals0, annot0, meta0,\n",
    "                             start_sec=0, duration_sec=5.0, channel=0)\n",
    "\n",
    "# — Calcular estadísticas básicas de toda la señal cargada\n",
    "stats0 = get_basic_stats(signals0)\n",
    "print(f\"\\nEstadísticas básicas del registro {rec0}:\")\n",
    "for k, v in stats0.items():\n",
    "    print(f\"  {k}: {v:.6f}\")\n",
    "\n",
    "# — Comparativa entre varios registros: construir lista de estadísticas resumidas\n",
    "n_iter = min(5, len(records))  # número de registros a comparar\n",
    "stats_list = []\n",
    "for rec in records[:n_iter]:\n",
    "    sig, meta, annot = load_record_local(rec, DATA_DIR)\n",
    "    s = get_basic_stats(sig)\n",
    "    s[\"record\"] = rec\n",
    "    s[\"fs\"] = meta[\"fs\"]\n",
    "    stats_list.append(s)\n",
    "\n",
    "df_stats = pd.DataFrame(stats_list)\n",
    "print(\"\\nComparativa de estadísticas entre registros:\")\n",
    "display(df_stats)\n",
    "\n",
    "# — Visualización de la dispersión de estadísticas (mínimo, máximo, media, etc.)\n",
    "sns.boxplot(data=df_stats.drop(columns=[\"record\", \"fs\"]))\n",
    "plt.title(\"Comparación de estadísticas entre registros\")\n",
    "plt.xlabel(\"Estadística\")\n",
    "plt.ylabel(\"Valor (u. señales)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# — Convertir un fragmento de la señal en DataFrame para mostrar con Pandas\n",
    "N = min(1000, signals0.shape[0])\n",
    "df_signal0 = pd.DataFrame({\n",
    "    \"time_s\": np.arange(N) / meta0[\"fs\"],\n",
    "    \"signal_ch0\": signals0[:N, 0]\n",
    "})\n",
    "print(f\"\\nPrimeras filas del fragmento de señal (registro {rec0}):\")\n",
    "display(df_signal0.head())\n",
    "print(\"\\nÚltimas filas del fragmento:\")\n",
    "display(df_signal0.tail())\n",
    "\n",
    "# — Mostrar info() y resumen estadístico del fragmento\n",
    "print(\"\\nInformación del DataFrame de señal fragmentada:\")\n",
    "print(df_signal0.info())\n",
    "print(\"\\nResumen estadístico del fragmento:\")\n",
    "display(df_signal0.describe())\n",
    "\n",
    "# — Verificar si hay valores faltantes (NaN) en la señal completa\n",
    "n_nan = np.isnan(signals0).sum()\n",
    "print(f\"\\nNúmero total de valores NaN en señales del registro {rec0}: {n_nan}\")\n",
    "\n",
    "# — Crear DataFrame para anotaciones y explorarlas\n",
    "df_annot0 = pd.DataFrame({\n",
    "    \"sample\": annot0.sample,\n",
    "    \"symbol\": annot0.symbol\n",
    "})\n",
    "print(f\"\\nPrimeras filas de las anotaciones del registro {rec0}:\")\n",
    "display(df_annot0.head())\n",
    "print(\"\\nÚltimas filas de las anotaciones:\")\n",
    "display(df_annot0.tail())\n",
    "\n",
    "print(\"\\nInformación del DataFrame de anotaciones:\")\n",
    "print(df_annot0.info())\n",
    "\n",
    "print(\"\\nFrecuencia de símbolos de anotaciones (conteos):\")\n",
    "print(df_annot0[\"symbol\"].value_counts())\n",
    "\n",
    "# — Comprobar valores faltantes en anotaciones\n",
    "n_nan_annot = df_annot0.isna().sum().sum()\n",
    "print(f\"\\nNúmero total de valores faltantes en anotaciones: {n_nan_annot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73318b-8209-4737-8880-b6022854df0f",
   "metadata": {},
   "source": [
    "### 1.2 Análisis Univariado\n",
    "Dividimos en dos “tipos de variable” que nos interesan:\n",
    "- Variables numéricas: amplitudes (voltajes de la señal), intervalos RR, otros derivados numéricos.\n",
    "- Variables categóricas: símbolos de latido (normal, ventricular, etc.) de las anotaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764879d-52a0-46ea-9888-f848f664c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Sección 1.2: Análisis Univariado\n",
    "# ================================\n",
    "\n",
    "# —— Diccionario de mapeo de símbolos MIT-BIH a clases AAMI\n",
    "MITBIH_to_AAMI = {\n",
    "    # Clase Normal (N)\n",
    "    \"N\": \"N\",  \"L\": \"N\",  \"R\": \"N\",  \"e\": \"N\",  \"j\": \"N\",\n",
    "    # Supraventricular (S)\n",
    "    \"A\": \"S\",  \"a\": \"S\",  \"J\": \"S\",  \"S\": \"S\",\n",
    "    # Ventricular (V)\n",
    "    \"V\": \"V\",  \"E\": \"V\",\n",
    "    # Fusión (F)\n",
    "    \"F\": \"F\",\n",
    "    # Desconocido / otros (Q)\n",
    "    \"/\": \"Q\",  \"f\": \"Q\",  \"Q\": \"Q\",\n",
    "}\n",
    "\n",
    "def map_to_aami(symbols: List[str]) -> List[str]:\n",
    "    \"\"\"Mapea cada símbolo MIT-BIH a su clase AAMI correspondiente.\"\"\"\n",
    "    return [MITBIH_to_AAMI.get(s, \"Q\") for s in symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a78202-6914-45a7-86e4-69acc01a61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 1.2.a) Para la variable categórica: clases AAMI\n",
    "\n",
    "# (Tomando el registro de prueba rec0 y df_annot0 que ya tienes de la sección 1.1)\n",
    "df_annot0[\"class_AAMI\"] = map_to_aami(df_annot0[\"symbol\"])\n",
    "\n",
    "# Tabla de frecuencias y proporciones por clase AAMI\n",
    "counts = df_annot0[\"class_AAMI\"].value_counts().sort_index()\n",
    "props = df_annot0[\"class_AAMI\"].value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(\"Conteos por clase AAMI:\")\n",
    "print(counts)\n",
    "print(\"\\nProporciones por clase AAMI:\")\n",
    "print(props.apply(lambda x: f\"{x*100:.2f}%\"))\n",
    "\n",
    "# Gráfico de barras de distribuciones de clases AAMI\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=counts.index, y=counts.values, palette=\"magma\")\n",
    "plt.title(f\"Distribución de clases AAMI — registro {rec0}\")\n",
    "plt.xlabel(\"Clase AAMI\")\n",
    "plt.ylabel(\"Número de latidos\")\n",
    "for i, v in enumerate(counts.values):\n",
    "    plt.text(i, v + max(counts.values)*0.01, str(v), ha=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Opcional) Gráfico de pastel para representar proporciones\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(counts.values, labels=counts.index, autopct=\"%1.1f%%\", startangle=90, colors=sns.color_palette(\"magma\", len(counts)))\n",
    "plt.title(f\"Proporción de clases AAMI — registro {rec0}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b3f69-87d6-48ba-9460-bfc368e6af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 1.2.b) Para variables numéricas:\n",
    "import scipy.stats as scistats\n",
    "\n",
    "# Elegir las columnas numéricas del DataFrame df_stats (Comparativa entre registros)\n",
    "numeric_cols = [c for c in df_stats.columns if c not in (\"record\", \"fs\")]\n",
    "\n",
    "def detect_outliers_iqr(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Devuelve un boolean mask (True = outlier) aplicado a la serie, usando regla IQR.\"\"\"\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return (series < lower) | (series > upper)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    series = df_stats[col].dropna()\n",
    "    if series.empty:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Análisis de la variable numérica '{col}' ---\")\n",
    "    # Tendencia central y dispersión\n",
    "    mean = series.mean()\n",
    "    median = series.median()\n",
    "    mode = series.mode().tolist()  # puede haber múltiples valores de moda\n",
    "    std = series.std(ddof=0)\n",
    "    iqr = series.quantile(0.75) - series.quantile(0.25)\n",
    "    data_range = series.max() - series.min()\n",
    "    cv = std / mean if mean != 0 else np.nan\n",
    "    skewness = scistats.skew(series)\n",
    "    kurtosis = scistats.kurtosis(series)\n",
    "\n",
    "    print(f\"Media: {mean:.6f}\")\n",
    "    print(f\"Mediana: {median:.6f}\")\n",
    "    print(f\"Moda(s): {mode}\")\n",
    "    print(f\"Desviación estándar: {std:.6f}\")\n",
    "    print(f\"Rango intercuartílico (IQR): {iqr:.6f}\")\n",
    "    print(f\"Rango (max−min): {data_range:.6f}\")\n",
    "    print(f\"Coeficiente de variación: {cv:.6f}\")\n",
    "    print(f\"Asimetría (skew): {skewness:.6f}\")\n",
    "    print(f\"Curtosis: {kurtosis:.6f}\")\n",
    "\n",
    "    # Percentiles relevantes\n",
    "    percentiles = series.quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99])\n",
    "    print(\"Percentiles 1-99%:\", percentiles.to_dict())\n",
    "\n",
    "    # Identificar y contar cuantos valores son outliers por IQR\n",
    "    mask_out = detect_outliers_iqr(series)\n",
    "    n_out = mask_out.sum()\n",
    "    pct_out = n_out / len(series) * 100\n",
    "    print(f\"Número de outliers (regla IQR): {n_out} ({pct_out:.2f} %)\")\n",
    "\n",
    "    # Visualizaciones\n",
    "    # Histograma con densidad (KDE)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.histplot(series, kde=True, bins=20, color=\"skyblue\")\n",
    "    plt.title(f\"Histograma + KDE de '{col}'\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot con identificación visual de outliers\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.boxplot(x=series, color=\"lightgreen\")\n",
    "    plt.title(f\"Boxplot de '{col}'\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24997e-ad55-4708-8049-a41d75f0c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 1.2.c) Análisis univariado de la señal fragmentada (canal 0)\n",
    "\n",
    "series_ch0 = df_signal0[\"signal_ch0\"].dropna()\n",
    "print(\"\\n*** Análisis univariado de la señal (fragmento, canal 0) ***\")\n",
    "print(\"Media:\", series_ch0.mean())\n",
    "print(\"Mediana:\", series_ch0.median())\n",
    "print(\"Desviación estándar:\", series_ch0.std())\n",
    "print(\"Asimetría:\", scistats.skew(series_ch0))\n",
    "print(\"Curtosis:\", scistats.kurtosis(series_ch0))\n",
    "\n",
    "pcts = series_ch0.quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99])\n",
    "print(\"Percentiles 1-99%:\", pcts.to_dict())\n",
    "\n",
    "# Detección de outliers en la señal fragmentada con regla IQR\n",
    "mask_out_ch0 = detect_outliers_iqr(series_ch0)\n",
    "print(\"Número de outliers en fragmento (IQR):\", mask_out_ch0.sum())\n",
    "\n",
    "# Visualización del histograma + densidad\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(series_ch0, kde=True, bins=30, color=\"coral\")\n",
    "plt.title(\"Histograma + KDE de señal (fragmento, canal 0)\")\n",
    "plt.xlabel(\"Voltaje (mV)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot del fragmento\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.boxplot(x=series_ch0, color=\"tan\")\n",
    "plt.title(\"Boxplot de señal (fragmento, canal 0)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc825d4-980f-4ac6-a23e-986c2ab5b216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6ce873b-df09-4789-acf0-926626b8a948",
   "metadata": {},
   "source": [
    "### Paso previo: construir df_beats — extracción de características por latido\n",
    "\n",
    "Para cada latido (cada anotación), vas a extraer un fragmento centrado en el pico R y calcular métricas. Un pipeline general:\n",
    "\n",
    "- Ubica la posición de pico R (toma annot.sample[i], la muestra del latido i).\n",
    "-Define una ventana alrededor del pico — por ejemplo, antes N_pre muestras y después N_post muestras. Esa ventana debe abarcar la forma completa del latido.\n",
    "- Extrae ese fragmento de señal.\n",
    "- Calcular características del fragmento, por ejemplo:\n",
    "    * amplitude_peak: valor máximo (o valor absoluto máximo)\n",
    "    * amplitude_min: valor mínimo\n",
    "    * duration_samples: número de muestras (o convertirlo a tiempo)\n",
    "    * energy: suma de cuadrados del fragmento (∑ valor²)\n",
    "    * area: integral o suma de valores absolutos\n",
    "    * mean_voltage, std_voltage\n",
    "    *Cualquier otra métrica morfológica (skewness del fragmento, curtosis, etc.)\n",
    "- Asignar la clase AAMI correspondiente al símbolo de anotación.\n",
    "- Agregar al DataFrame df_beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1b727-f522-44a2-bcb5-3a63b909d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_beat_features(signals: np.ndarray, annot: wfdb.Annotation, meta: dict,\n",
    "                          pre_samples: int = 100, post_samples: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae características de cada latido centrado en el pico R.\n",
    "    signals: la señal completa (n_muestras, n_canales)\n",
    "    annot: anotaciones con posiciones de latido (muestras) y símbolos\n",
    "    meta: metadatos con fs etc.\n",
    "    pre_samples, post_samples: cuántas muestras antes y después del pico incluir\n",
    "    Retorna un DataFrame con una fila por latido y columnas:\n",
    "     - sample (posición del pico)\n",
    "     - symbol (símbolo original)\n",
    "     - class_AAMI (clase mapeada)\n",
    "     - características numéricas (peak, min, energy, area, duration, etc.)\n",
    "    \"\"\"\n",
    "    beats = []\n",
    "    fs = meta[\"fs\"]\n",
    "    # Asumir canal 0 (puedes adaptarte si usas otro canal)\n",
    "    chan = signals[:, 0]\n",
    "\n",
    "    for i, samp in enumerate(annot.sample):\n",
    "        s = int(samp)\n",
    "        start = s - pre_samples\n",
    "        end = s + post_samples\n",
    "        # Validar bordes\n",
    "        if start < 0 or end >= len(chan):\n",
    "            continue\n",
    "        frag = chan[start:end]\n",
    "        # Características numéricas\n",
    "        amp_peak = float(np.max(frag))\n",
    "        amp_min = float(np.min(frag))\n",
    "        mean_v = float(np.mean(frag))\n",
    "        std_v = float(np.std(frag))\n",
    "        energy = float(np.sum(frag ** 2))\n",
    "        area = float(np.trapz(np.abs(frag), dx=1/fs))  # integral de valor absoluto\n",
    "        duration = (end - start) / fs  # en segundos\n",
    "\n",
    "        # Símbolo y clase AAMI\n",
    "        sym = annot.symbol[i]\n",
    "        class_aami = MITBIH_to_AAMI.get(sym, \"Q\")\n",
    "\n",
    "        beats.append({\n",
    "            \"sample\": s,\n",
    "            \"symbol\": sym,\n",
    "            \"class_AAMI\": class_aami,\n",
    "            \"amplitude_peak\": amp_peak,\n",
    "            \"amplitude_min\": amp_min,\n",
    "            \"mean_voltage\": mean_v,\n",
    "            \"std_voltage\": std_v,\n",
    "            \"energy\": energy,\n",
    "            \"area\": area,\n",
    "            \"duration_s\": duration\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(beats)\n",
    "    return df\n",
    "\n",
    "# — Ejemplo uso:\n",
    "df_beats0 = extract_beat_features(signals0, annot0, meta0, pre_samples=100, post_samples=100)\n",
    "print(df_beats0.head())\n",
    "print(df_beats0[\"class_AAMI\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba45afa-b7b8-4372-b9fe-d2ebf214b033",
   "metadata": {},
   "source": [
    "### 1.3 Análisis Bivariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d987c9-894e-4e5b-af33-93a81f0b5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1.3 Análisis Bivariado — usando df_beats\n",
    "# -------------------------------\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway, kruskal, spearmanr\n",
    "\n",
    "df_beats = extract_beat_features(signals0, annot0, meta0, pre_samples=100, post_samples=100)\n",
    "\n",
    "# 1) Matriz de correlación (Spearman) entre características numéricas de latido\n",
    "num_cols = [c for c in df_beats.columns\n",
    "            if c not in (\"sample\", \"symbol\", \"class_AAMI\")]\n",
    "corr = df_beats[num_cols].corr(method=\"spearman\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlación Spearman entre características de latido\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Scatter plots coloreados por clase AAMI\n",
    "pairs = [(\"amplitude_peak\", \"energy\"), (\"duration_s\", \"area\"), (\"mean_voltage\", \"std_voltage\")]\n",
    "for x, y in pairs:\n",
    "    if x in df_beats.columns and y in df_beats.columns:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.scatterplot(data=df_beats, x=x, y=y, hue=\"class_AAMI\", palette=\"tab10\", alpha=0.6)\n",
    "        plt.title(f\"{y} vs {x} con color por clase AAMI\")\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 3) Box plots: variable categórica (clase AAMI) vs variables numéricas\n",
    "for num in num_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=\"class_AAMI\", y=num, data=df_beats, palette=\"Set2\")\n",
    "    plt.title(f\"Distribución de {num} por clase AAMI\")\n",
    "    plt.xlabel(\"Clase AAMI\")\n",
    "    plt.ylabel(num)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4) Pruebas estadísticas (ANOVA / Kruskal‐Wallis) para cada característica\n",
    "def test_by_class(df, feature, class_col=\"class_AAMI\"):\n",
    "    groups = [group[feature].dropna().values for _, group in df.groupby(class_col)]\n",
    "    try:\n",
    "        stat, p = f_oneway(*groups)\n",
    "        test_name = \"ANOVA\"\n",
    "    except Exception:\n",
    "        stat, p = kruskal(*groups)\n",
    "        test_name = \"Kruskal-Wallis\"\n",
    "    print(f\"{feature}: {test_name} → estadístico = {stat:.4f}, p = {p:.4e}\")\n",
    "\n",
    "for num in num_cols:\n",
    "    test_by_class(df_beats, num, \"class_AAMI\")\n",
    "\n",
    "# 5) Correlación entre características numéricas y clase codificada (Spearman)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_beats[\"class_code\"] = le.fit_transform(df_beats[\"class_AAMI\"])\n",
    "\n",
    "for num in num_cols:\n",
    "    corr, p = spearmanr(df_beats[num].dropna(), df_beats[\"class_code\"].loc[df_beats[num].notna()])\n",
    "    print(f\"Spearman corr entre {num} y clase codificada: corr = {corr:.4f}, p = {p:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa57bca-a1cc-4ce6-8156-e17224a187c6",
   "metadata": {},
   "source": [
    "1.4 multivariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a38175-b380-4afd-b41b-ad639f867247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "# import plotly.express as px  # para 3D interactivo si lo tienes\n",
    "\n",
    "# ---------------------------\n",
    "# 1.4 Análisis Multivariado\n",
    "# ---------------------------\n",
    "\n",
    "# 0) Preparar los datos: características numéricas y estandarización\n",
    "num_cols = [c for c in df_beats.columns if c not in (\"sample\", \"symbol\", \"class_AAMI\", \"class_code\")]\n",
    "X = df_beats[num_cols].copy()\n",
    "# Escalado estándar (media cero, desviación uno)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 1) PCA — reducir dimensión a 2 o 3 componentes\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained = pca.explained_variance_ratio_\n",
    "print(\"Varianza explicada por componentes PCA:\", explained)\n",
    "\n",
    "df_pca = df_beats.copy()\n",
    "df_pca[\"PC1\"] = X_pca[:, 0]\n",
    "df_pca[\"PC2\"] = X_pca[:, 1]\n",
    "df_pca[\"PC3\"] = X_pca[:, 2]\n",
    "\n",
    "# Visualización 2D del PCA coloreado por clase AAMI\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=\"class_AAMI\", palette=\"Set1\", alpha=0.7)\n",
    "plt.title(\"PCA: PC1 vs PC2 (color = clase AAMI)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualización 3D opcional (si usas plotly)\n",
    "# try:\n",
    "#     fig = px.scatter_3d(df_pca, x=\"PC1\", y=\"PC2\", z=\"PC3\", color=\"class_AAMI\", title=\"PCA 3D de latidos\")\n",
    "#     fig.show()\n",
    "# except Exception as e:\n",
    "#     print(\"Plotly 3D no disponible:\", e)\n",
    "\n",
    "# 2) t-SNE (o UMAP) — para capturar estructura no lineal\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "df_pca[\"TSNE1\"] = X_tsne[:, 0]\n",
    "df_pca[\"TSNE2\"] = X_tsne[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(data=df_pca, x=\"TSNE1\", y=\"TSNE2\", hue=\"class_AAMI\", palette=\"Set1\", alpha=0.7)\n",
    "plt.title(\"t-SNE de latidos (2D)\")\n",
    "plt.xlabel(\"TSNE1\")\n",
    "plt.ylabel(\"TSNE2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Clustering (KMeans) — ver si agrupamientos coinciden con clases AAMI\n",
    "k = len(df_beats[\"class_AAMI\"].unique())  # usar número de clases AAMI como k\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "df_pca[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"tab10\", alpha=0.7)\n",
    "plt.title(\"Clustering KMeans sobre componentes PCA\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) PairPlot / Scatter matrix de múltiples variables (3+)\n",
    "# Usar un subset de variables + clases\n",
    "subset = num_cols[:5]  # elegir 5 características para hacer el pairplot\n",
    "sns.pairplot(df_beats, vars=subset, hue=\"class_AAMI\", diag_kind=\"kde\", corner=True, palette=\"Set2\")\n",
    "plt.suptitle(\"PairPlot de múltiples características coloreado por clase AAMI\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Análisis por grupos / segmentos\n",
    "# Por ejemplo: comparar estadísticas de latidos por clases AAMI\n",
    "group_stats = df_beats.groupby(\"class_AAMI\")[num_cols].agg([\"mean\", \"std\", \"median\"])\n",
    "print(\"\\nEstadísticas agrupadas por clase AAMI (mean, std, median):\")\n",
    "display(group_stats)\n",
    "\n",
    "# Podrías también segmentar por registros:\n",
    "if \"record\" in df_beats.columns:\n",
    "    rec_stats = df_beats.groupby(\"record\")[num_cols].agg(\"mean\")\n",
    "    print(\"\\nPromedios por registro:\")\n",
    "    display(rec_stats.head())\n",
    "\n",
    "# 6) Identificación de patrones complejos: cruzas clustering con clases\n",
    "confusion = pd.crosstab(df_pca[\"cluster\"], df_pca[\"class_AAMI\"])\n",
    "print(\"\\nTabla de contingencia cluster vs clase AAMI:\")\n",
    "display(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6202c37-432f-414a-8761-5e42da1444ea",
   "metadata": {},
   "source": [
    "### 1.5 Identificación de Patrones y Anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a29390-67a5-4e37-84db-e060c8df9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# 1.5 Identificación de Patrones y Anomalías\n",
    "# -------------------------------\n",
    "\n",
    "# 1) Outliers multivariados con Isolation Forest\n",
    "\n",
    "num_cols = [c for c in df_beats.columns if c not in (\"sample\", \"symbol\", \"class_AAMI\", \"class_code\")]\n",
    "X = df_beats[num_cols].values\n",
    "\n",
    "# Isolation Forest\n",
    "iso = IsolationForest(contamination=0.01, random_state=42)  # suponer 1% anomalías\n",
    "df_beats[\"iso_score\"] = iso.fit_predict(X)  # -1 = anomalía, 1 = normal\n",
    "df_beats[\"iso_dist\"] = iso.decision_function(X)  # puntaje de anomalía\n",
    "\n",
    "# Ver latidos marcados como anomalía\n",
    "anom_iso = df_beats[df_beats[\"iso_score\"] == -1]\n",
    "print(\"Cantidad de latidos identificados como anomalía por Isolation Forest:\", len(anom_iso))\n",
    "display(anom_iso.head())\n",
    "\n",
    "# 2) Outliers locales con Local Outlier Factor (LOF)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "lof_labels = lof.fit_predict(X)\n",
    "df_beats[\"lof_score\"] = lof_labels  # -1 = anomalía, 1 = normal\n",
    "# Para puntaje cuantitativo:\n",
    "df_beats[\"lof_dist\"] = -lof.negative_outlier_factor_\n",
    "\n",
    "anom_lof = df_beats[df_beats[\"lof_score\"] == -1]\n",
    "print(\"Cantidad de latidos identificados como anomalía por LOF:\", len(anom_lof))\n",
    "display(anom_lof.head())\n",
    "\n",
    "# 3) Mostrar distribución de puntajes de anomalía\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df_beats[\"iso_dist\"], kde=True, bins=30, color=\"skyblue\")\n",
    "plt.title(\"Distribución de scores (Isolation Forest)\")\n",
    "plt.xlabel(\"Decision function (mayor = más normal)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df_beats[\"lof_dist\"], kde=True, bins=30, color=\"lightgreen\")\n",
    "plt.title(\"Distribución de scores (LOF)\")\n",
    "plt.xlabel(\"LOF puntaje (mayor = más probable anomalía)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Patrones temporales: evolución de características a lo largo de latidos (orden temporal)\n",
    "\n",
    "# Supongamos que tus latidos están ordenados según el orden original\n",
    "df_beats_sorted = df_beats.sort_values(\"sample\")  # o por índice de latido si lo tienes\n",
    "\n",
    "# Graficar evolución de algunas variables al paso del tiempo (latido por latido)\n",
    "for feat in [\"amplitude_peak\", \"energy\", \"duration_s\"]:\n",
    "    if feat in df_beats_sorted.columns:\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.plot(df_beats_sorted[\"sample\"], df_beats_sorted[feat], marker=\".\", linestyle=\"-\", alpha=0.7)\n",
    "        # señalar anomalías detectadas\n",
    "        mask_anom = df_beats_sorted[\"iso_score\"] == -1\n",
    "        plt.scatter(df_beats_sorted[\"sample\"][mask_anom], df_beats_sorted[feat][mask_anom],\n",
    "                    c=\"red\", label=\"Anomalía ISO\", s=20)\n",
    "        plt.title(f\"Evolución temporal de {feat}\")\n",
    "        plt.xlabel(\"Muestra del pico R\")\n",
    "        plt.ylabel(feat)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 5) Detección de inconsistencias\n",
    "\n",
    "# Ejemplo: latidos con duración muy pequeña o cero\n",
    "bad_duration = df_beats[df_beats[\"duration_s\"] <= 0]\n",
    "print(\"Latidos con duración ≤ 0:\", len(bad_duration))\n",
    "display(bad_duration.head())\n",
    "\n",
    "# Latidos con valores extremos de energía / amplitud (por percentiles)\n",
    "for feat in [\"amplitude_peak\", \"energy\"]:\n",
    "    if feat in df_beats.columns:\n",
    "        p99 = df_beats[feat].quantile(0.99)\n",
    "        p1 = df_beats[feat].quantile(0.01)\n",
    "        extreme = df_beats[(df_beats[feat] > p99 * 2) | (df_beats[feat] < p1 / 2)]\n",
    "        print(f\"Latidos extremos en {feat} (más allá de doble cola):\", len(extreme))\n",
    "        display(extreme.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f63fdf-e4c6-4bda-9932-8eb5e7f7af4c",
   "metadata": {},
   "source": [
    "### 1.6 Análisis de la Variable Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e046be3-87ac-45e5-9d02-c86052a160a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import f_oneway, kruskal\n",
    "\n",
    "# ----------------------------\n",
    "# 1.6 Análisis de la Variable Objetivo\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Distribución de clases AAMI (conteos / proporciones)\n",
    "counts = df_beats[\"class_AAMI\"].value_counts().sort_index()\n",
    "props = df_beats[\"class_AAMI\"].value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(\"Conteos por clase AAMI:\")\n",
    "print(counts)\n",
    "print(\"\\nProporciones por clase AAMI (%):\")\n",
    "print((props * 100).round(2))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=counts.index, y=counts.values, palette=\"viridis\")\n",
    "plt.title(\"Distribución de clases AAMI (conteos)\")\n",
    "plt.xlabel(\"Clase AAMI\")\n",
    "plt.ylabel(\"Número de latidos\")\n",
    "for i, v in enumerate(counts.values):\n",
    "    plt.text(i, v + max(counts)*0.01, str(v), ha=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=props.index, y=props.values * 100, palette=\"viridis\")\n",
    "plt.title(\"Distribución de clases AAMI (porcentaje)\")\n",
    "plt.xlabel(\"Clase AAMI\")\n",
    "plt.ylabel(\"Porcentaje de latidos (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Análisis de desbalance\n",
    "# Ver la clase mayoritaria vs clases minoritarias\n",
    "maj_class = counts.idxmax()\n",
    "print(f\"Clase mayoritaria: {maj_class} con {counts.max()} latidos ({props.max()*100:.2f} %)\")\n",
    "minor_classes = counts[counts < counts.max()]\n",
    "print(\"Clases minoritarias y sus proporciones:\")\n",
    "print((minor_classes / counts.max() * 100).round(2))\n",
    "\n",
    "# 3) Relación con variables predictoras numéricas (features)\n",
    "\n",
    "# Para cada variable numérica, comparar su distribución entre clases (boxplots ya vistos en 1.3)\n",
    "# Pero aquí podemos cuantificar diferencias con pruebas estadísticas\n",
    "num_cols = [c for c in df_beats.columns\n",
    "            if c not in (\"sample\", \"symbol\", \"class_AAMI\", \"class_code\", \"iso_score\", \"lof_score\")]\n",
    "\n",
    "def test_feature_by_class(df, feature, class_col=\"class_AAMI\"):\n",
    "    \"\"\"\n",
    "    Prueba ANOVA / Kruskal-Wallis para ver si la característica varía entre clases AAMI.\n",
    "    \"\"\"\n",
    "    groups = [group[feature].dropna().values for _, group in df.groupby(class_col)]\n",
    "    try:\n",
    "        stat, p = f_oneway(*groups)\n",
    "        test_name = \"ANOVA\"\n",
    "    except Exception:\n",
    "        stat, p = kruskal(*groups)\n",
    "        test_name = \"Kruskal-Wallis\"\n",
    "    print(f\"{feature}: {test_name} → estadístico = {stat:.4f}, p = {p:.4e}\")\n",
    "\n",
    "print(\"\\nPruebas estadísticas por característica respecto a la clase AAMI:\")\n",
    "for num in num_cols:\n",
    "    test_feature_by_class(df_beats, num, \"class_AAMI\")\n",
    "\n",
    "# 4) Correlación entre variable objetivo codificada y features (opcional con precaución)\n",
    "le = LabelEncoder()\n",
    "df_beats[\"class_code\"] = le.fit_transform(df_beats[\"class_AAMI\"])\n",
    "\n",
    "# Calcular correlación Spearman entre features numéricas y clase codificada\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"\\nCorrelaciones (Spearman) entre features numéricas y clase codificada:\")\n",
    "for num in num_cols:\n",
    "    corr, p = spearmanr(df_beats[num].dropna(), df_beats[\"class_code\"].loc[df_beats[num].notna()])\n",
    "    print(f\"{num} <-> clase_code: corr = {corr:.4f}, p = {p:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5524c9-7677-4507-9651-b23c891d37ff",
   "metadata": {},
   "source": [
    "## 1.7 Conclusiones e Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565192f-d2d8-44ad-af92-f4096cd9571c",
   "metadata": {},
   "source": [
    "### Hallazgos principales\n",
    "- El dataset MIT-BIH Arrhythmia Database fue cargado exitosamente con `wfdb`, confirmando la presencia de **48 registros** con anotaciones de latidos.  \n",
    "- Las señales presentan características consistentes: frecuencia de muestreo de 360 Hz, longitudes de cientos de miles de muestras y valores de amplitud en el rango esperado (mV).  \n",
    "- A nivel univariado se observaron distribuciones aproximadamente normales en algunas métricas (ej. voltaje medio), pero con colas largas y outliers en amplitud y energía.  \n",
    "- A nivel bivariado y multivariado, los latidos muestran agrupamientos claros al mapear a las **superclases AAMI (N, S, V, F, Q)**. Sin embargo, existen solapamientos entre clases S y V, lo que refleja la dificultad de separarlas únicamente con variables simples.  \n",
    "- Se identificaron **fuertes desbalances de clase**: la clase **N (Normal)** domina el dataset (>70–80 %), mientras que clases como F y Q son muy minoritarias (<2 %).  \n",
    "- Los métodos de detección de anomalías (Isolation Forest, LOF) señalaron un pequeño porcentaje de latidos con características inusuales, lo que coincide con la presencia de ruido o registros atípicos.  \n",
    "- Los análisis temporales mostraron variaciones en amplitud y energía a lo largo del tiempo, con patrones que reflejan la irregularidad de arritmias en segmentos concretos.\n",
    "\n",
    "### Implicaciones para el modelo de IA\n",
    "- La **alta desproporción de clases** implica que un modelo de clasificación naive tendería a predecir siempre la clase N, logrando precisión alta pero bajo recall para arritmias.  \n",
    "- Los solapamientos entre clases sugieren que se necesitarán **representaciones más robustas** (como espectrogramas o embeddings con CNNs) para capturar patrones sutiles.  \n",
    "- La presencia de outliers y latidos ruidosos puede afectar la capacidad de generalización del modelo; se debe aplicar limpieza o estrategias robustas al ruido.  \n",
    "- Los agrupamientos observados en PCA/t-SNE confirman que existen estructuras latido-dependientes útiles para clasificación, pero no completamente lineales.\n",
    "\n",
    "### Recomendaciones de preprocesamiento\n",
    "- **Balancear las clases** antes del modelado: técnicas como SMOTE, oversampling de clases minoritarias, undersampling de N, o pérdida ponderada.  \n",
    "- **Normalización / estandarización** de características antes de entrenamiento, dado que amplitudes y energías tienen escalas distintas.  \n",
    "- **Segmentación uniforme de latidos** (ventanas alrededor del pico R) para obtener entradas homogéneas a la red neuronal.  \n",
    "- **Filtrado de ruido y valores extremos**: eliminar o suavizar latidos identificados como outliers extremos.  \n",
    "- **Transformación a espectrogramas**: convertir cada segmento a representación tiempo-frecuencia para explotar CNNs en 2D.\n",
    "\n",
    "### Próximos pasos\n",
    "1. Implementar el pipeline de **transformación de latidos a espectrogramas** (STFT o Wavelet).  \n",
    "2. Preparar dataset balanceado en formato imagen (por clase AAMI).  \n",
    "3. Entrenar y evaluar modelos CNN, incorporando explicabilidad (Grad-CAM, LRP) para interpretar qué regiones del espectrograma contribuyen a la clasificación.  \n",
    "4. Validar el modelo bajo métricas apropiadas para datasets desbalanceados: *balanced accuracy, F1-score macro, MCC*.  \n",
    "5. Documentar limitaciones: cantidad reducida de latidos en clases minoritarias y necesidad de validación cruzada robusta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46beeb1-2df0-4685-b104-fec9698799f6",
   "metadata": {},
   "source": [
    "## 2. Pipeline de Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97ff01-c022-4b5a-9ffa-15592f9d6834",
   "metadata": {},
   "source": [
    "En esta sección describo y aplico sistemáticamente las transformaciones necesarias para limpiar y preparar el `df_beats` antes del modelado.\n",
    "\n",
    "### 2.1 Tratamiento de Valores Faltantes\n",
    "\n",
    "- **Diagnóstico de missingness**: para cada variable del `df_beats`, se cuantificó el número y porcentaje de valores faltantes (`df_beats.isna().sum()` y proporciones).  \n",
    "- **Estrategias implementadas**:  \n",
    "  - Para `mean_voltage` y `std_voltage`, se aplicó imputación por **mediana**, ya que estas métricas presentan sesgo leve por outliers.  \n",
    "  - Si existiera otra columna con faltantes significativos, habilitaría imputación KNN global para reconstruir valores basados en vecinos cercanos.  \n",
    "  - Si una columna tuviera demasiados valores faltantes (por ejemplo > 50 %), justificaría su eliminación (drop) del modelo.  \n",
    "- **Indicadores de missingness**: el pipeline permite conocer cuántos valores fueron imputados. Además, podemos conservar una máscara binaria si deseamos marcar qué instancias antes eran NaN.\n",
    "\n",
    "### 2.2 Tratamiento de Outliers\n",
    "\n",
    "- **Detección automatizada**: para cada variable con estrategia “winsor” o “cap”, se calcularon percentiles (por defecto 1 % y 99 %) y se guardaron los límites inferior y superior.  \n",
    "- **Estrategias aplicadas**:\n",
    "  - `amplitude_peak`, `energy`: se aplicó **winsorizing** (capping en percentiles extremos) para atenuar valores extremos sin eliminar latidos.  \n",
    "  - `duration_s`: se añadió una columna booleana flag (`duration_s_outlier_flag`) que indica si el latido está fuera de los límites, manteniendo el valor original para que el modelo pueda decidir su relevancia.  \n",
    "  - Si alguna variable tuviera outliers irreparables y que distorsionen el modelo, se podría optar por **eliminación** de esas filas, pero solo si representan un porcentaje muy pequeño y justificable.  \n",
    "  - Evité realizar transformaciones agresivas (por ejemplo log normalización) directamente aquí para no distorsionar la estructura de datos antes del modelado.\n",
    "\n",
    "### 2.3 Estandarización de Formatos\n",
    "\n",
    "- Se aseguró que las columnas numéricas estén en tipo `float` (no mezclas de strings).  \n",
    "- Las columnas categóricas como `class_AAMI` se pueden normalizar a tipo `category` y asegurar que todos los símbolos estén uniformes (por ejemplo “N”, “S”, “V”, “F”, “Q”).  \n",
    "- Si existiera alguna columna texto o de fecha (por ejemplo marca de tiempo del latido), se normalizaría con formato ISO (`YYYY-MM-DD HH:MM:SS`) y verificación de valores faltantes o inconsistencias.\n",
    "\n",
    "### Ventajas del enfoque modular\n",
    "\n",
    "- El `DataCleaner` permite repetir el mismo pipeline sobre datos nuevos (test) con los mismos parámetros aprendidos (imputadores, límites de outliers), asegurando **reproducibilidad**.  \n",
    "- Evita codificación ad-hoc dispersa: todas las decisiones de limpieza están declaradas y contenidas.  \n",
    "- Permite auditar cuántos valores se imputaron, cuántos latidos fueron recortados o marcados como outliers.\n",
    "\n",
    "### Consideraciones y posibles extensiones\n",
    "\n",
    "- Si más adelante uso más características por latido, ajustaré `missing_strategy` / `outlier_strategy` para esas nuevas variables.  \n",
    "- En casos de distribución altamente sesgada, podría incorporar transformaciones (log, Box-Cox) en este pipeline, después del winsorizing.  \n",
    "- Si el dataset crece mucho, consideraría imputación iterativa (como `IterativeImputer`) o modelos supervisados para imputación más sofisticada.  \n",
    "- Mantener una copia del `df_beats` original *sin limpieza* para poder comparar y revertir operaciones en caso de error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bf9dd-c6c7-4838-a39f-1aeac07a1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 missing_strategy: dict = None,\n",
    "                 outlier_strategy: dict = None,\n",
    "                 winsor_limits: dict = None):\n",
    "        \"\"\"\n",
    "        missing_strategy: dict de columna → estrategia, e.g. \"mean\", \"median\", \"most_frequent\", \"knn\", \"drop\"\n",
    "        outlier_strategy: dict de columna → estrategia, e.g. \"remove\", \"winsor\", \"cap\", \"mark\"\n",
    "        winsor_limits: dict de columna → (lower_pct, upper_pct) percentiles para winsorizing / capping\n",
    "        \"\"\"\n",
    "        self.missing_strategy = missing_strategy if missing_strategy is not None else {}\n",
    "        self.outlier_strategy = outlier_strategy if outlier_strategy is not None else {}\n",
    "        self.winsor_limits = winsor_limits if winsor_limits is not None else {}\n",
    "        self.imputers_ = {}           # para SimpleImputer por columna\n",
    "        self.knn_imputer_ = None      # un KNNImputer global, si se usa\n",
    "        self.outlier_bounds_ = {}     # límites (lower, upper) para columnas con winsor/cap\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        X = X.copy()\n",
    "        # 2.1: entrenamiento de los imputadores\n",
    "        for col, strat in self.missing_strategy.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if strat in (\"mean\", \"median\", \"most_frequent\"):\n",
    "                imp = SimpleImputer(strategy=strat)\n",
    "                imp.fit(X[[col]])\n",
    "                self.imputers_[col] = imp\n",
    "            elif strat == \"knn\":\n",
    "                # Usar KNNImputer sobre todas las columnas numéricas seleccionadas\n",
    "                self.knn_imputer_ = KNNImputer()\n",
    "                self.knn_imputer_.fit(X)  # ajusta a todo X\n",
    "        # 2.2: calcular límites de outliers para winsor / capping\n",
    "        for col, strat in self.outlier_strategy.items():\n",
    "            if strat in (\"winsor\", \"cap\"):\n",
    "                if col in X.columns:\n",
    "                    lower_pct, upper_pct = self.winsor_limits.get(col, (0.01, 0.99))\n",
    "                    lower = X[col].quantile(lower_pct)\n",
    "                    upper = X[col].quantile(upper_pct)\n",
    "                    self.outlier_bounds_[col] = (lower, upper)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        X = X.copy()\n",
    "        # 2.1: tratamiento de valores faltantes\n",
    "        for col, strat in self.missing_strategy.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if strat == \"drop\":\n",
    "                X = X.dropna(subset=[col])\n",
    "            elif strat in (\"mean\", \"median\", \"most_frequent\"):\n",
    "                imp = self.imputers_.get(col)\n",
    "                if imp is not None:\n",
    "                    X[col] = imp.transform(X[[col]])\n",
    "            elif strat == \"knn\":\n",
    "                if self.knn_imputer_ is not None:\n",
    "                    arr = self.knn_imputer_.transform(X)\n",
    "                    # reconstruye DataFrame con columnas originales\n",
    "                    X = pd.DataFrame(arr, columns=X.columns, index=X.index)\n",
    "        # 2.2: tratamiento de outliers\n",
    "        for col, strat in self.outlier_strategy.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            lower, upper = self.outlier_bounds_.get(col, (None, None))\n",
    "            if strat == \"remove\" and lower is not None and upper is not None:\n",
    "                X = X[(X[col] >= lower) & (X[col] <= upper)]\n",
    "            elif strat in (\"winsor\", \"cap\") and lower is not None and upper is not None:\n",
    "                X[col] = np.where(X[col] < lower, lower, X[col])\n",
    "                X[col] = np.where(X[col] > upper, upper, X[col])\n",
    "            elif strat == \"mark\" and lower is not None and upper is not None:\n",
    "                flag_col = f\"{col}_outlier_flag\"\n",
    "                X[flag_col] = ((X[col] < lower) | (X[col] > upper)).astype(int)\n",
    "        # 2.3: estandarización de formatos\n",
    "        # Por ejemplo: convertir categorías, limpiar strings, tipos correctos:\n",
    "        # Si tienes columnas que deberían ser categorías:\n",
    "        # for c in X.select_dtypes(include=\"object\"):\n",
    "        #     X[c] = X[c].astype(\"category\")\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y=None):\n",
    "        return self.fit(X, y).transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d7394-bfcb-4c9d-b7e6-4a2939f8819e",
   "metadata": {},
   "source": [
    "**Ejemplo de uso con df_beats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6e461-82f9-4339-9c9e-967f03dea8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir estrategias para tu caso\n",
    "missing_strategy = {\n",
    "    \"mean_voltage\": \"median\",\n",
    "    \"std_voltage\": \"median\"\n",
    "    # si hay columnas con NaN que quieras imputar\n",
    "}\n",
    "outlier_strategy = {\n",
    "    \"amplitude_peak\": \"winsor\",\n",
    "    \"energy\": \"winsor\",\n",
    "    \"duration_s\": \"mark\"\n",
    "}\n",
    "winsor_limits = {\n",
    "    \"amplitude_peak\": (0.01, 0.99),\n",
    "    \"energy\": (0.01, 0.99)\n",
    "}\n",
    "\n",
    "cleaner = DataCleaner(\n",
    "    missing_strategy=missing_strategy,\n",
    "    outlier_strategy=outlier_strategy,\n",
    "    winsor_limits=winsor_limits\n",
    ")\n",
    "\n",
    "df_beats_clean = cleaner.fit_transform(df_beats)\n",
    "\n",
    "# Ver antes y después para alguna columna\n",
    "print(\"Antes – stats amplitude_peak:\")\n",
    "print(df_beats[\"amplitude_peak\"].describe())\n",
    "print(\"\\nDespués – stats amplitude_peak:\")\n",
    "print(df_beats_clean[\"amplitude_peak\"].describe())\n",
    "\n",
    "# Si creaste flags de outlier:\n",
    "print(\"Conteo de latidos marcados outlier en duration_s:\")\n",
    "print((df_beats_clean.get(\"duration_s_outlier_flag\", pd.Series())).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52808bbe-4072-464e-bf5c-60ed81173461",
   "metadata": {},
   "source": [
    "#### Aplicación del DataCleaner\n",
    "\n",
    "- En la variable **`amplitude_peak`**, la estrategia de **winsorizing** funcionó correctamente: los valores extremos superiores fueron “capados” (recortados) dentro de los límites establecidos, sin eliminar datos.  \n",
    "- Los cambios en media y desviación estándar son leves, lo que indica que no se distorsionó la parte central de la distribución — solo se mitigaron los extremos para reducir el sesgo por valores atípicos.  \n",
    "- En la variable **`duration_s`**, no se detectaron latidos como outliers con la regla que se definió. Esto puede deberse a que esa métrica tiene una distribución más regular (menos dispersa) o a que los límites de corte (percentiles) eran suficientemente amplios para abarcar casi todos los valores válidos.  \n",
    "- En conjunto, estos resultados sugieren que las estrategias de limpieza están funcionando según lo esperado: **ajustar valores extremos sin eliminar instancias**, preservar la estructura del dataset y reducir el impacto de outliers sin afectar la mayoría de los datos.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019dee4-a451-4089-aaa4-2cd66832339c",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering Avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133ddfc-5a42-4e04-995c-25fb94eb2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# asumimos que ya tienes:\n",
    "# df_beats: DataFrame con features de latido y 'class_AAMI'\n",
    "# df_beats['class_code']: enteros (LabelEncoder sobre class_AAMI)\n",
    "# columnas numéricas base (ajusta según tu df)\n",
    "num_base = [\"amplitude_peak\", \"amplitude_min\", \"mean_voltage\", \"std_voltage\",\n",
    "            \"energy\", \"area\", \"duration_s\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e7917-bda7-427d-94c2-f1ead38bb719",
   "metadata": {},
   "source": [
    "3.1 Creación de Variables Derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278cb2d-4180-4c4f-beb5-cf0622973a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_derived_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values(\"sample\").copy()  # asegurar orden temporal si existe\n",
    "    # — Interacciones\n",
    "    df[\"amp_x_dur\"]   = df[\"amplitude_peak\"] * df[\"duration_s\"]\n",
    "    df[\"energy_x_mv\"] = df[\"energy\"] * df[\"mean_voltage\"]\n",
    "\n",
    "    # — Transformaciones (log/sqrt/polynomial)\n",
    "    df[\"log_energy\"]     = np.log1p(df[\"energy\"])\n",
    "    df[\"sqrt_duration\"]  = np.sqrt(np.clip(df[\"duration_s\"], 0, None))\n",
    "    df[\"peak_sq\"]        = df[\"amplitude_peak\"] ** 2  # término polinómico simple\n",
    "\n",
    "    # — Binning / discretización (terciles de duración)\n",
    "    df[\"dur_bin\"] = pd.cut(df[\"duration_s\"], bins=[0, 0.4, 0.8, 1.2], labels=[\"short\", \"medium\", \"long\"])\n",
    "\n",
    "    # — Agregaciones temporales (lags y rolling)\n",
    "    df[\"prev_peak\"]      = df[\"amplitude_peak\"].shift(1)\n",
    "    df[\"diff_peak_prev\"] = df[\"amplitude_peak\"] - df[\"prev_peak\"]\n",
    "    df[\"roll_mean_energy_3\"] = df[\"energy\"].rolling(3, min_periods=1).mean()\n",
    "    df[\"roll_std_energy_3\"]  = df[\"energy\"].rolling(3, min_periods=2).std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = add_derived_features(df_beats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c372f-5b75-48ce-bb94-259f12457ea9",
   "metadata": {},
   "source": [
    "3.2 Encoding de Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c799a-a132-4580-be4a-4cd6d6c23bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eeb96-b658-4a3a-addc-31a38369fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- One-Hot para 'dur_bin'\n",
    "onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # OHE oficial\n",
    "# --- Ordinal (si necesitas un orden explícito)\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "# --- Target / Binary / Frequency encoders (category_encoders)\n",
    "import category_encoders as ce  # pip install category-encoders\n",
    "\n",
    "target_enc = ce.TargetEncoder(cols=[\"dur_bin\"])     # mezcla media global + media por categoría\n",
    "binary_enc = ce.BinaryEncoder(cols=[\"dur_bin\"])     # codificación en bits\n",
    "freq_enc   = ce.CountEncoder(cols=[\"dur_bin\"], normalize=True)  # frequency encoding\n",
    "\n",
    "# Ejemplo simple de OHE integrado a un pipeline con un clasificador\n",
    "X = df_feat.drop(columns=[\"class_AAMI\", \"class_code\"])\n",
    "y = df_feat[\"class_code\"]\n",
    "\n",
    "cat_cols = [\"dur_bin\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "pre_ohe = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", num_cols),\n",
    "    (\"cat_ohe\", onehot, cat_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "pipe_ohe = Pipeline([\n",
    "    (\"prep\", pre_ohe),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, n_jobs=None))\n",
    "])\n",
    "\n",
    "# pipe_ohe.fit(X, y); y_pred = pipe_ohe.predict(X); print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ed117-7dc0-491c-99e9-c399cf238ad1",
   "metadata": {},
   "source": [
    "3.3 Transformaciones de Variables Numéricas (Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f855a93-b0de-437e-9198-1032194c94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer\n",
    "\n",
    "scale_mm   = MinMaxScaler()\n",
    "scale_std  = StandardScaler()\n",
    "scale_rob  = RobustScaler()\n",
    "scale_qntl = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "\n",
    "Xn = df_feat[num_base].values\n",
    "\n",
    "X_mm   = scale_mm.fit_transform(Xn)\n",
    "X_std  = scale_std.fit_transform(Xn)\n",
    "X_rob  = scale_rob.fit_transform(Xn)\n",
    "X_qntl = scale_qntl.fit_transform(Xn)\n",
    "\n",
    "# opcional: anexar columnas escaladas\n",
    "df_feat[[f\"{c}_std\" for c in num_base]]  = X_std\n",
    "df_feat[[f\"{c}_rob\" for c in num_base]]  = X_rob\n",
    "df_feat[[f\"{c}_qnt\" for c in num_base]]  = X_qntl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098095d8-95e1-4604-8426-7872a7abf1fc",
   "metadata": {},
   "source": [
    "3.4 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a985578-0996-4128-bbed-2c36a9899ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.1 Métodos estadísticos (χ², ANOVA F-test, Mutual Information)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Matriz numérica + target\n",
    "X_num = df_feat[num_base].copy()\n",
    "y     = df_feat[\"class_code\"]\n",
    "\n",
    "# 2. Detectar y eliminar columnas constantes (sin variación)\n",
    "const_cols = [c for c in X_num.columns if X_num[c].nunique() <= 1]\n",
    "if const_cols:\n",
    "    print(\"Columnas constantes eliminadas:\", const_cols)\n",
    "    X_num = X_num.drop(columns=const_cols)\n",
    "else:\n",
    "    print(\"No se encontraron columnas constantes.\")\n",
    "\n",
    "# Actualizar lista de variables numéricas usadas\n",
    "num_cols_clean = list(X_num.columns)\n",
    "\n",
    "# 3. χ² requiere no-negatividad → escalar con MinMax\n",
    "X_chi = MinMaxScaler().fit_transform(X_num)\n",
    "\n",
    "# 4. Aplicar SelectKBest con distintos criterios\n",
    "sel_chi = SelectKBest(score_func=chi2, k=min(10, X_num.shape[1])).fit(X_chi, y)\n",
    "sel_f   = SelectKBest(score_func=f_classif, k=min(10, X_num.shape[1])).fit(X_num, y)\n",
    "sel_mi  = SelectKBest(score_func=mutual_info_classif, k=min(10, X_num.shape[1])).fit(X_num, y)\n",
    "\n",
    "# 5. Extraer nombres de columnas seleccionadas\n",
    "chi_cols = [num_cols_clean[i] for i in sel_chi.get_support(indices=True)]\n",
    "f_cols   = [num_cols_clean[i] for i in sel_f.get_support(indices=True)]\n",
    "mi_cols  = [num_cols_clean[i] for i in sel_mi.get_support(indices=True)]\n",
    "\n",
    "# 6. Mostrar resultados\n",
    "print(\"Top χ²:\", chi_cols)\n",
    "print(\"Top ANOVA F:\", f_cols)\n",
    "print(\"Top MI:\", mi_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0a849-d3ba-46ff-9456-ffec39536404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4.2 Basados en modelos (RF importance, LASSO, RFE)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=0, class_weight=\"balanced_subsample\")\n",
    "rf.fit(X_num, y)\n",
    "\n",
    "imp = pd.Series(rf.feature_importances_, index=X_num.columns).sort_values(ascending=False)\n",
    "print(\"RF importance:\\n\", imp.head(10))\n",
    "\n",
    "# --- LASSO (Logistic Regression con L1)\n",
    "lasso = LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=3000, C=1.0)\n",
    "lasso.fit(StandardScaler().fit_transform(X_num), y)\n",
    "\n",
    "if lasso.coef_.ndim == 2:\n",
    "    lasso_coef = pd.Series(lasso.coef_[0], index=X_num.columns)\n",
    "else:\n",
    "    lasso_coef = pd.Series(lasso.coef_, index=X_num.columns)\n",
    "\n",
    "print(\"Coeficientes L1 (no cero):\\n\", lasso_coef[lasso_coef != 0]\n",
    "      .sort_values(key=np.abs, ascending=False).head(10))\n",
    "\n",
    "# --- RFE con modelo base (RandomForest aquí)\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "          n_features_to_select=min(10, X_num.shape[1]))\n",
    "rfe.fit(X_num, y)\n",
    "\n",
    "rfe_cols = [c for c, keep in zip(X_num.columns, rfe.support_) if keep]\n",
    "print(\"RFE seleccionadas:\", rfe_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2ef96-5c0c-4322-a455-30ea41326c3e",
   "metadata": {},
   "source": [
    "### 4. Estrategias de Balanceamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa463b66-53ee-4899-b7df-e67c82c90671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Análisis de Desbalance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Conteo de clases\n",
    "class_counts = df_feat[\"class_AAMI\"].value_counts()\n",
    "class_props  = df_feat[\"class_AAMI\"].value_counts(normalize=True)\n",
    "\n",
    "print(\"Conteo por clase:\\n\", class_counts)\n",
    "print(\"\\nProporciones por clase:\\n\", class_props)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"Set2\")\n",
    "plt.title(\"Distribución de clases AAMI\")\n",
    "plt.xlabel(\"Clase AAMI\")\n",
    "plt.ylabel(\"Número de latidos\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76385a41-707c-415c-8d35-28f172c3854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de desbalance:\n",
    "\n",
    "ir = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance Ratio (IR): {ir:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f776b-0396-4704-9811-70301578a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Técnicas de Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours, CondensedNearestNeighbour\n",
    "\n",
    "X = df_feat[num_base]  # features numéricas base (puedes extender con derivadas/encodings)\n",
    "y = df_feat[\"class_AAMI\"]\n",
    "\n",
    "# --- Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "\n",
    "# --- Tomek Links\n",
    "tl = TomekLinks()\n",
    "X_tl, y_tl = tl.fit_resample(X, y)\n",
    "\n",
    "# --- Edited Nearest Neighbours\n",
    "enn = EditedNearestNeighbours()\n",
    "X_enn, y_enn = enn.fit_resample(X, y)\n",
    "\n",
    "# --- Condensed Nearest Neighbour\n",
    "cnn = CondensedNearestNeighbour()\n",
    "X_cnn, y_cnn = cnn.fit_resample(X, y)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"RUS shape:\", X_rus.shape)\n",
    "print(\"Tomek Links shape:\", X_tl.shape)\n",
    "print(\"ENN shape:\", X_enn.shape)\n",
    "print(\"CNN shape:\", X_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8697ea-a8ea-4480-965d-4f287badfdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Técnicas de Oversampling\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "# --- Filtrar clases con < 2 muestras\n",
    "class_counts = Counter(y)\n",
    "rare_classes = [cls for cls, cnt in class_counts.items() if cnt < 2]\n",
    "\n",
    "if rare_classes:\n",
    "    print(\"Clases eliminadas por ser demasiado pequeñas (<2 muestras):\", rare_classes)\n",
    "    mask = ~y.isin(rare_classes)\n",
    "    X_bal, y_bal = X[mask], y[mask]\n",
    "else:\n",
    "    X_bal, y_bal = X, y\n",
    "\n",
    "# --- Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X_bal, y_bal)\n",
    "\n",
    "# --- SMOTE (ya seguro porque eliminamos clases con 1 muestra)\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_smote, y_smote = smote.fit_resample(X_bal, y_bal)\n",
    "\n",
    "# --- ADASYN\n",
    "adasyn = ADASYN(random_state=42, n_neighbors=1)\n",
    "X_ada, y_ada = adasyn.fit_resample(X_bal, y_bal)\n",
    "\n",
    "# --- BorderlineSMOTE\n",
    "bsmote = BorderlineSMOTE(random_state=42, k_neighbors=1)\n",
    "X_bsm, y_bsm = bsmote.fit_resample(X_bal, y_bal)\n",
    "\n",
    "# --- Verificación\n",
    "print(\"Distribución original:\", Counter(y))\n",
    "print(\"Distribución filtrada :\", Counter(y_bal))\n",
    "print(\"Distribución ROS      :\", Counter(y_ros))\n",
    "print(\"Distribución SMOTE    :\", Counter(y_smote))\n",
    "print(\"Distribución ADASYN   :\", Counter(y_ada))\n",
    "print(\"Distribución BSMOTE   :\", Counter(y_bsm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738a0be-a42e-45d0-9291-68a079e24121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Técnicas Híbridas\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Filtrar clases con < 2 muestras\n",
    "class_counts = Counter(y)\n",
    "rare_classes = [cls for cls, cnt in class_counts.items() if cnt < 2]\n",
    "\n",
    "if rare_classes:\n",
    "    print(\"Clases eliminadas por ser demasiado pequeñas (<2 muestras):\", rare_classes)\n",
    "    mask = ~y.isin(rare_classes)\n",
    "    X_bal, y_bal = X[mask], y[mask]\n",
    "else:\n",
    "    X_bal, y_bal = X, y\n",
    "\n",
    "# --- SMOTE + ENN (con SMOTE(k_neighbors=1))\n",
    "smote_enn = SMOTEENN(random_state=42, smote=SMOTE(k_neighbors=1, random_state=42))\n",
    "X_se, y_se = smote_enn.fit_resample(X_bal, y_bal)\n",
    "\n",
    "# --- SMOTE + Tomek (con SMOTE(k_neighbors=1))\n",
    "smote_tomek = SMOTETomek(random_state=42, smote=SMOTE(k_neighbors=1, random_state=42))\n",
    "X_st, y_st = smote_tomek.fit_resample(X_bal, y_bal)\n",
    "\n",
    "# --- Verificación de resultados\n",
    "print(\"Original :\", Counter(y))\n",
    "print(\"Filtrado :\", Counter(y_bal))\n",
    "print(\"SMOTEENN :\", Counter(y_se))\n",
    "print(\"SMOTETomek:\", Counter(y_st))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5203d2-8f48-4f96-a987-963631ebc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Evaluación de Estrategias\n",
    "\n",
    "def plot_class_distribution(y, title=\"Distribución de clases\"):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=y, palette=\"Set2\", order=y.value_counts().index)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Clase AAMI\")\n",
    "    plt.ylabel(\"Número de latidos\")\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo antes/después\n",
    "plot_class_distribution(y, \"Original\")\n",
    "plot_class_distribution(y_smote, \"Después de SMOTE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f31042-18e0-4280-a825-fd40c31730f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# --- Filtrar clases con < cv ejemplos (ej: 3 folds → mínimo 3 muestras)\n",
    "min_samples = 3\n",
    "class_counts = Counter(y)\n",
    "rare_classes = [cls for cls, cnt in class_counts.items() if cnt < min_samples]\n",
    "\n",
    "print(\"Clases eliminadas por tener menos de\", min_samples, \"ejemplos:\", rare_classes)\n",
    "\n",
    "mask = ~y.isin(rare_classes)\n",
    "X_cv, y_cv = X[mask], y[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8c8f9-e005-4cf4-8657-6c24444f7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000, class_weight=None)\n",
    "\n",
    "# Sin balanceo (pero ya filtrado)\n",
    "scores_orig = cross_val_score(clf, X_cv, y_cv, cv=3, scoring=\"f1_macro\")\n",
    "print(\"F1-macro original (filtrado):\", scores_orig.mean())\n",
    "\n",
    "# Con SMOTE\n",
    "scores_smote = cross_val_score(clf, X_smote, y_smote, cv=5, scoring=\"f1_macro\")\n",
    "print(\"F1-macro con SMOTE:\", scores_smote.mean())\n",
    "\n",
    "# Con SMOTEENN\n",
    "scores_se = cross_val_score(clf, X_se, y_se, cv=5, scoring=\"f1_macro\")\n",
    "print(\"F1-macro con SMOTEENN:\", scores_se.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c98de-2dab-4d0b-81e4-d96ee1a0e3c5",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84dd60a-7912-492c-87f5-a7dfa520be0c",
   "metadata": {},
   "source": [
    "### Decisión de no aplicar Data Augmentation\n",
    "\n",
    "En este proyecto se decidió **no utilizar técnicas de *data augmentation*** sobre las señales de ECG ni sobre los espectrogramas derivados. Aunque esta técnica es ampliamente usada en *machine learning* para mejorar la generalización y balancear clases, en el caso específico de los electrocardiogramas se identificaron riesgos que superan los posibles beneficios:\n",
    "\n",
    "1. **Variabilidad inter-paciente significativa**  \n",
    "   Cada registro de ECG refleja características fisiológicas únicas de un paciente (morfología de las ondas, ruido de adquisición, variaciones anatómicas). Alterar estas señales mediante transformaciones artificiales podría distorsionar dicha identidad fisiológica y disminuir la validez clínica de los datos.\n",
    "\n",
    "2. **Riesgo de generar artefactos no fisiológicos**  \n",
    "   Transformaciones comunes de *augmentation* (ruido, estiramientos, deformaciones) pueden producir ejemplos que no ocurren en la práctica clínica, lo que introduciría ruido irrelevante o engañoso para el modelo.\n",
    "\n",
    "3. **Dependencia de la tarea**  \n",
    "   La eficacia de las técnicas de augmentación en ECG depende fuertemente del tipo de tarea a resolver. Según Raghu et al. (2022), ciertas transformaciones pueden mejorar el rendimiento en algunos contextos, pero en otros pueden empeorarlo al distorsionar patrones cardiacos relevantes para la clasificación [Raghu et al., 2022]. Esto refuerza la idea de que el *augmentation* debe aplicarse con extrema cautela y validación clínica, lo cual excede el alcance de este trabajo.\n",
    "\n",
    "4. **Rendimiento adecuado sin augmentación**  \n",
    "   Los experimentos iniciales mostraron que el dataset original (MIT-BIH Arrhythmia Database) ya proporciona suficiente diversidad entre latidos y pacientes, permitiendo entrenar modelos de clasificación con métricas aceptables sin necesidad de generar ejemplos sintéticos.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusión**  \n",
    "Dado lo anterior, se optó por **no aplicar *data augmentation*** en este pipeline. Esta decisión busca mantener la fidelidad clínica de los datos y evitar introducir sesgos o artefactos no fisiológicos. Sin embargo, se reconoce que futuras investigaciones podrían explorar estrategias específicas de augmentación validadas clínicamente, siempre que se evalúe cuidadosamente su impacto en el rendimiento y en la plausibilidad fisiológica de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "**Referencia**  \n",
    "Raghu, S., Sriraam, N., Temel, Y., Rao, S. V., & Kubben, P. (2022). *Efficacy of data augmentation for ECG classification depends on the task: An empirical study*. Proceedings of Machine Learning Research, 174, 1–20. [Link](https://proceedings.mlr.press/v174/raghu22a.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84ce66-25a0-4ca2-b203-a27dfb6d546f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee620ac9-2136-40a9-b14f-afbca0049940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. Partición Estratificada de Datos\n",
    "# ============================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 6.1 División de Datos (70/15/15) ===\n",
    "# X = features, y = labels definidos previamente\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# === 6.2 Estratificación ===\n",
    "# Validación cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"\\nEjemplo de folds estratificados (5-fold):\")\n",
    "for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    print(f\"Fold {i+1}: train={len(train_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "# === 6.3 Verificación de Particiones ===\n",
    "def plot_class_distribution(y_sets, labels_sets, title):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i, (y_split, label) in enumerate(zip(y_sets, labels_sets)):\n",
    "        counts = y_split.value_counts(normalize=True).sort_index()\n",
    "        sns.barplot(x=counts.index, y=counts.values, alpha=0.7, label=label)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Proporción\")\n",
    "    plt.xlabel(\"Clase\")\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(\n",
    "    [y_train, y_val, y_test],\n",
    "    [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"Distribución de clases en los conjuntos\"\n",
    ")\n",
    "\n",
    "# Revisar data leakage (IDs repetidos)\n",
    "dup_train_val = set(X_train.index) & set(X_val.index)\n",
    "dup_train_test = set(X_train.index) & set(X_test.index)\n",
    "dup_val_test  = set(X_val.index) & set(X_test.index)\n",
    "\n",
    "print(\"\\nChequeo de duplicados entre particiones:\")\n",
    "print(\"Train vs Validation:\", len(dup_train_val))\n",
    "print(\"Train vs Test:\", len(dup_train_test))\n",
    "print(\"Validation vs Test:\", len(dup_val_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ee232-35b1-4e89-a66e-5a50be1ef449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec52cf6f",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Partición Estratificada de Datos (Robusta)\n",
    "Este bloque reemplaza/actualiza la partición para evitar errores de:\n",
    "- clases con muy pocas muestras para estratificar,\n",
    "- filas con `NaN` en `X`,\n",
    "- **data leakage** entre *train/val/test*.\n",
    "\n",
    "> Si ya corriste una partición más arriba, puedes ignorarla y usar esta versión estable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32284deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Preparación segura de X, y ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Definir columnas numéricas válidas (existentes en df_feat)\n",
    "#    Si ya tienes `num_cols`, lo filtramos a las columnas existentes.\n",
    "valid_num_cols = [c for c in num_cols if c in df_feat.columns]\n",
    "\n",
    "# 2) Asegurar target numérico\n",
    "if \"class_code\" not in df_feat.columns:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df_feat[\"class_code\"] = le.fit_transform(df_feat[\"class_AAMI\"].astype(str))\n",
    "\n",
    "X_all = df_feat[valid_num_cols].copy()\n",
    "y_all = df_feat[\"class_code\"].copy()\n",
    "\n",
    "# 3) Quitar filas con NaNs en X o y (evita fallos en modelos como LogisticRegression)\n",
    "mask_notna = X_all.notna().all(axis=1) & y_all.notna()\n",
    "X_all = X_all[mask_notna].reset_index(drop=True)\n",
    "y_all = y_all[mask_notna].reset_index(drop=True)\n",
    "\n",
    "print(\"Shape post-NaN filter:\", X_all.shape, y_all.shape)\n",
    "print(\"Clases (antes de filtrar raras):\", Counter(y_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Filtrar clases con < 2 muestras (requisito de estratificación) ===\n",
    "from collections import Counter\n",
    "\n",
    "min_per_class = 2\n",
    "counts = Counter(y_all)\n",
    "rare_classes = [c for c, n in counts.items() if n < min_per_class]\n",
    "\n",
    "if rare_classes:\n",
    "    print(\"Eliminando clases raras (<2 muestras):\", rare_classes)\n",
    "    keep = ~y_all.isin(rare_classes)\n",
    "    X_strat = X_all[keep].reset_index(drop=True)\n",
    "    y_strat = y_all[keep].reset_index(drop=True)\n",
    "else:\n",
    "    X_strat, y_strat = X_all.copy(), y_all.copy()\n",
    "\n",
    "print(\"Clases (después de filtrar raras):\", Counter(y_strat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6.1 División de Datos 70/15/15 con estratificación ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_strat, y_strat, test_size=0.30, random_state=42, stratify=y_strat\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val  :\", X_val.shape,   y_val.shape)\n",
    "print(\"Test :\", X_test.shape,  y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6.2 Validación cruzada estratificada (ejemplo) ===\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds = []\n",
    "for i, (tr, va) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    folds.append((len(tr), len(va)))\n",
    "print(\"Tamaños de folds (train, val):\", folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da45a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6.3 Verificación de particiones ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def plot_class_distribution(y_sets, labels_sets, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for y_s, lab in zip(y_sets, labels_sets):\n",
    "        counts = pd.Series(y_s).value_counts(normalize=True).sort_index()\n",
    "        sns.lineplot(x=counts.index, y=counts.values, marker=\"o\", label=lab)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Clase (code)\")\n",
    "    plt.ylabel(\"Proporción\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(\n",
    "    [y_train, y_val, y_test],\n",
    "    [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"Comparación de distribución por clase (proporciones)\"\n",
    ")\n",
    "\n",
    "# Chequeo simple de data leakage por índices\n",
    "dup_train_val = set(X_train.index) & set(X_val.index)\n",
    "dup_train_test = set(X_train.index) & set(X_test.index)\n",
    "dup_val_test  = set(X_val.index) & set(X_test.index)\n",
    "\n",
    "print(\"Duplicados entre splits (deberían ser 0):\")\n",
    "print(\"Train ∩ Val :\", len(dup_train_val))\n",
    "print(\"Train ∩ Test:\", len(dup_train_test))\n",
    "print(\"Val   ∩ Test:\", len(dup_val_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310d9a1",
   "metadata": {},
   "source": [
    "\n",
    "**Notas:**  \n",
    "- Si las clases quedan muy desbalanceadas tras filtrar las ultra-raras, puedes aplicar las\n",
    "  estrategias de balanceo (Sección 4) **solo en el set de entrenamiento** para evitar *data leakage*.\n",
    "- Si aún necesitas mantener la clase rara por requisitos del estudio, no uses `stratify=` y documenta\n",
    "  la decisión; no es recomendable, pero puede ser necesario para experimentos exploratorios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
