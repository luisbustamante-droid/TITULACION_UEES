{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# EDA — MIT-BIH Arrhythmia Database (Kaggle CSV + annotations)\n",
        "\n",
        "**Proyecto:** *Clasificación explicable de arritmias cardíacas a partir de ECG transformados en espectrogramas mediante CNNs*  \n",
        "\n",
        "**Objetivo del EDA:** Implementar un Análisis Exploratorio de Datos (EDA) completo sobre los registros MIT-BIH (canales MLII y V5) y sus anotaciones de latido/evento, para extraer **insights** que guíen el preprocesamiento y el modelado (segmentación en ventanas, normalización, manejo de clases, etc.)."
      ],
      "metadata": {
        "id": "q7F_e-Bv048m"
      },
      "id": "q7F_e-Bv048m"
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Importación de librerías\n",
        "# =====================\n",
        "import os, re, glob, math, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Estilo de gráficos\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
        "\n",
        "# Mostrar todas las columnas de pandas\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "print(\"Versions -> pandas:\", pd.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QGIJB3zt1HlQ",
        "outputId": "01c15ca8-d531-49a2-89ea-a0ae4e416bec"
      },
      "id": "QGIJB3zt1HlQ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versions -> pandas: 2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Configuración y utilidades\n",
        "\n",
        "- `DATA_DIR`: carpeta donde están los `.csv` y `annotations.txt` (formato Kaggle).  \n",
        "- `FS`: frecuencia de muestreo.  \n",
        "- Helpers para **cargar señales**, **cargar anotaciones** y **unificar formatos**.\n"
      ],
      "metadata": {
        "id": "iLzQEsBP1Slc"
      },
      "id": "iLzQEsBP1Slc"
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Configuración\n",
        "# =====================\n",
        "# Ajusta esta ruta a tu carpeta local (por ejemplo: r\"C:/Users/tu_usuario/Documents/Titulacion_UES/Notebooks/EDA-MIT-BIH-Arrhythmia-Database/kaggle_dataset\")\n",
        "DATA_DIR = Path(r\"C:\\Users\\luisa\\Documents\\Titulacion_UEES\\Notebooks\\EDA-MIT-BIH-Arrhythmia-Database\\kaggle_dataset\")\n",
        "\n",
        "# Frecuencia de muestreo típica de MIT-BIH\n",
        "FS = 360  # Hz\n",
        "\n",
        "# Intento de auto-descubrimiento si no existe la ruta por defecto\n",
        "if not DATA_DIR.exists():\n",
        "    # Busca recursivamente en el árbol actual alguna carpeta con csv+annotations\n",
        "    candidates = []\n",
        "    for p in Path(\".\").rglob(\"*\"):\n",
        "        try:\n",
        "            if p.is_dir():\n",
        "                has_csv = any(Path(p).glob(\"[0-9][0-9][0-9].csv\"))\n",
        "                has_ann = any(Path(p).glob(\"[0-9][0-9][0-9]annotations.txt\"))\n",
        "                if has_csv and has_ann:\n",
        "                    candidates.append(p)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if candidates:\n",
        "        DATA_DIR = candidates[0]\n",
        "        print(\"DATA_DIR auto-detectado en:\", DATA_DIR.resolve())\n",
        "    else:\n",
        "        print(\"⚠️ No se encontraron datos automáticamente. Ajusta DATA_DIR manualmente.\")\n",
        "\n",
        "DATA_DIR = DATA_DIR.resolve()\n",
        "DATA_DIR"
      ],
      "metadata": {
        "id": "FzScv0Yo1YvT"
      },
      "id": "FzScv0Yo1YvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Funciones robustas de carga\n",
        "# =====================\n",
        "def _clean_signal_df(df):\n",
        "    \"\"\"Normaliza nombres de columnas y tipos para el CSV de señales.\"\"\"\n",
        "    # Limpiar comillas simples, espacios extras en nombres de columnas\n",
        "    df.columns = [c.replace(\"'\", \"\").strip() for c in df.columns]\n",
        "    # Mapear 'sample #' (ignorando mayúsculas/minúsculas) a 'sample'\n",
        "    rename_map = {}\n",
        "    for c in df.columns:\n",
        "        cc = c.lower().replace(\" \", \"\").replace(\"#\", \"\")\n",
        "        if cc == \"sample\":\n",
        "            rename_map[c] = \"sample\"\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "    # Normalizar nombres de canales comunes\n",
        "    for c in list(df.columns):\n",
        "        clean = c.strip().upper().replace(\" \", \"\")\n",
        "        if clean in (\"MLII\", \"ML2\"):\n",
        "            df = df.rename(columns={c: \"MLII\"})\n",
        "        elif clean in (\"V1\",\"V2\",\"V4\",\"V5\",\"V6\"):\n",
        "            df = df.rename(columns={c: clean})\n",
        "    # Forzar numérico\n",
        "    for c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    # Quitar filas con NaN en columnas clave (al menos sample y al menos un canal)\n",
        "    # Definir columnas de canal\n",
        "    canal_cols = [col for col in df.columns if col in (\"MLII\",\"V1\",\"V2\",\"V4\",\"V5\",\"V6\")]\n",
        "    # dropna en sample y al menos un canal\n",
        "    df = df.dropna(subset=[\"sample\"] + canal_cols, how=\"all\").reset_index(drop=True)\n",
        "    # Asegurar tipo de sample\n",
        "    df['sample'] = df['sample'].astype(int)\n",
        "    # Crear columna de tiempo\n",
        "    df['time_sec'] = df['sample'] / FS\n",
        "    return df\n",
        "\n",
        "def read_signal_csv(csv_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Lee un CSV de señales con encabezados tipo 'sample #', 'MLII', 'V1'.\"\"\"\n",
        "    # Intento normal\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return _clean_signal_df(df)\n",
        "    # Podrías agregar fallback si no reconoce columnas, pero usualmente con tu encabezado será suficiente.\n",
        "\n",
        "def read_annotations_txt(ann_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Lee archivo de anotaciones con encabezado: Time   Sample #  Type  Sub Chan  Num Aux.\"\"\"\n",
        "    # Leer con read_csv separador por espacios múltiples, considerando encabezado\n",
        "    # Primero limpiamos encabezado\n",
        "    df = pd.read_csv(ann_path, sep=r\"\\s+\", engine='python')\n",
        "    # Si la columna “Sample #” está presente, renómbrala\n",
        "    df_cols = df.columns.tolist()\n",
        "    rename_ann = {}\n",
        "    for c in df_cols:\n",
        "        cc = c.lower().replace(\" \", \"\").replace(\"#\",\"\")\n",
        "        if cc == \"sample\":\n",
        "            rename_ann[c] = \"sample\"\n",
        "        elif cc == \"time\":\n",
        "            rename_ann[c] = \"Time\"\n",
        "        elif cc == \"type\":\n",
        "            rename_ann[c] = \"type\"\n",
        "        # las otras columnas Sub, Chan, Num, Aux pueden dejarse igual\n",
        "    if rename_ann:\n",
        "        df = df.rename(columns=rename_ann)\n",
        "    # Asegurar que columnas necesarias existen\n",
        "    required = [\"sample\", \"type\"]\n",
        "    for r in required:\n",
        "        if r not in df.columns:\n",
        "            raise RuntimeError(f\"Columna requerida '{r}' no encontrada en {ann_path}, columnas: {df.columns.tolist()}\")\n",
        "    # Convertir sample a entero\n",
        "    df['sample'] = pd.to_numeric(df['sample'], errors='coerce')\n",
        "    df = df.dropna(subset=['sample']).reset_index(drop=True)\n",
        "    df['sample'] = df['sample'].astype(int)\n",
        "    # Limpiar tipo\n",
        "    df['type'] = df['type'].astype(str).str.strip()\n",
        "    df = df[df['type'] != \"\"].reset_index(drop=True)\n",
        "    # Tiempo: si columna Time existe y no vacía, convertimos; si no, la generamos\n",
        "    if 'Time' in df.columns:\n",
        "        # A veces Time es formato hh:mm:ss.sss o mm:ss.xxx o cosas así;\n",
        "        # si no parsea bien, conviene usar sample/FS como respaldo\n",
        "        # Intentemos parseo robusto: si Time parsea como número flotante, bueno, sino ignorar\n",
        "        try:\n",
        "            df['Time_num'] = pd.to_numeric(df['Time'], errors='coerce')\n",
        "        except:\n",
        "            df['Time_num'] = np.nan\n",
        "        if df['Time_num'].isna().all():\n",
        "            df['Time'] = df['sample'] / FS\n",
        "        else:\n",
        "            df['Time'] = df['Time_num']\n",
        "        df = df.drop(columns=['Time_num'], errors='ignore')\n",
        "    else:\n",
        "        df['Time'] = df['sample'] / FS\n",
        "\n",
        "    # Seleccionar columnas finales\n",
        "    # Asegurar que al menos las columnas: sample, type, Sub, Chan, Num, Aux, Time\n",
        "    for col in ['Sub','Chan','Num','Aux']:\n",
        "        if col not in df.columns:\n",
        "            df[col] = np.nan\n",
        "    return df[['sample','type','Sub','Chan','Num','Aux','Time']].copy()\n",
        "\n",
        "\n",
        "def load_record(record_id: str, base_dir: Path = DATA_DIR) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Carga señales y anotaciones para un `record_id` (p. ej., '100').\"\"\"\n",
        "    csv_path = base_dir / f\"{record_id}.csv\"\n",
        "    ann_path = base_dir / f\"{record_id}annotations.txt\"\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"No existe: {csv_path}\")\n",
        "    if not ann_path.exists():\n",
        "        raise FileNotFoundError(f\"No existe: {ann_path}\")\n",
        "    sig = read_signal_csv(csv_path)\n",
        "    ann = read_annotations_txt(ann_path)\n",
        "    # Merge: marca el tipo de latido en las muestras exactas anotadas\n",
        "    sig = sig.copy()\n",
        "    sig['record'] = str(record_id)\n",
        "    ann = ann.copy()\n",
        "    ann['record'] = str(record_id)\n",
        "    sig = sig.merge(ann[['sample','type']], on='sample', how='left')\n",
        "    return sig, ann\n",
        "\n",
        "\n",
        "def list_record_ids(base_dir: Path = DATA_DIR) -> list[str]:\n",
        "    \"\"\"Lista IDs disponibles (a partir de archivos ###.csv).\"\"\"\n",
        "    ids = []\n",
        "    for p in base_dir.glob(\"[0-9][0-9][0-9].csv\"):\n",
        "        ids.append(p.stem)\n",
        "    return sorted(ids)"
      ],
      "metadata": {
        "id": "yvE3FViU1gYN"
      },
      "id": "yvE3FViU1gYN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}